{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZZXtg2nPkeab"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yspeo1Mm9wlY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import shap\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.manifold import Isomap\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from PIL import Image\n",
        "import io\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "sns.set(\n",
        "    { \"figure.figsize\": (8, 6) },\n",
        "    style='ticks',\n",
        "    color_codes=True,\n",
        "    font_scale=0.8\n",
        ")\n",
        "%config InlineBackend.figure_formats = set(('retina', 'svg'))\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa_rKz4q98uJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trX3Rb7HLHO2"
      },
      "outputs": [],
      "source": [
        "cars = pd.read_csv('/content/drive/MyDrive/adverts.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTEbwHeOYqks"
      },
      "source": [
        "# Data/Domain Understanding and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsYNKJvzYs-c"
      },
      "outputs": [],
      "source": [
        "cars.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_209K8gE-Hbo"
      },
      "outputs": [],
      "source": [
        "cars.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YHK35MlX-OtX"
      },
      "outputs": [],
      "source": [
        "cars.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r2Auwjfz-TCk"
      },
      "outputs": [],
      "source": [
        "cars.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EUO1X9i4-c_y"
      },
      "outputs": [],
      "source": [
        "cars.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SNsw6ELjIKPg"
      },
      "outputs": [],
      "source": [
        "cars.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c9e80NiuIZ6Z"
      },
      "outputs": [],
      "source": [
        "cars.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoOmKuS9ZWTs"
      },
      "outputs": [],
      "source": [
        "missing_values = cars.isnull().sum()\n",
        "print(missing_values)\n",
        "print((missing_values / len(cars)) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3beNW1SaR-yx"
      },
      "outputs": [],
      "source": [
        "for col in cars.columns:\n",
        "    print(f\"{col}: {cars[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V13BAaUHYvm5"
      },
      "outputs": [],
      "source": [
        "bins = [0, 100, 10000, 50000, 100000, 150000, 200000, 300000, float('inf')]\n",
        "mileage_labels = ['New', 'Very Low', 'Low', 'Medium', 'High', 'Very High', 'Extremely High', 'Ultra High']\n",
        "\n",
        "cars['mileage_bins'] = pd.cut(cars['mileage'], bins=bins, labels=mileage_labels, right=False)\n",
        "\n",
        "print(cars[['mileage', 'mileage_bins']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqEZwnkXZB3P"
      },
      "outputs": [],
      "source": [
        "price_bins = [0, 5000, 20000, 60000, 100000, 200000, float('inf')]\n",
        "price_labels = ['Very Low', 'Low', 'Medium', 'High', 'Luxury', 'Ultra Luxury']\n",
        "\n",
        "cars['price_bins'] = pd.cut(cars['price'], bins=price_bins, labels=price_labels, right=False)\n",
        "\n",
        "print(cars[['price', 'price_bins']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQZbd2CgkbVH"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "cars['mileage_bins'].value_counts().sort_index().plot(\n",
        "    kind='bar',\n",
        "    color='green',\n",
        "    edgecolor='black',\n",
        "    ax=axes[0]\n",
        ")\n",
        "axes[0].set_title('Distribution of Vehicles by Mileage Bins', fontsize=16, fontweight='bold')\n",
        "axes[0].set_xlabel('Mileage Bins', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "cars['price_bins'].value_counts().sort_index().plot(\n",
        "    kind='bar',\n",
        "    color='orange',\n",
        "    edgecolor='black',\n",
        "    ax=axes[1]\n",
        ")\n",
        "axes[1].set_title('Distribution of Vehicles by Price Bins', fontsize=16, fontweight='bold')\n",
        "axes[1].set_xlabel('Price Bins', fontsize=12, fontweight='bold')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ligq7jNKkk8M"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    pd.crosstab(cars['mileage_bins'], cars['price_bins']),\n",
        "    annot=True, fmt='d', cmap='plasma'  # تغییر پالت رنگ به 'coolwarm'\n",
        ")\n",
        "plt.title('Relationship Between Mileage Bins and Price Bins', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Price Bins', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Mileage Bins', fontsize=12, fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5tIjgoN9AEpN"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'year_of_registration'\n",
        "\n",
        "year_of_registration_stats = cars['year_of_registration'].describe()\n",
        "\n",
        "unique_years = cars['year_of_registration'].nunique()\n",
        "\n",
        "year_of_registration_freq = cars['year_of_registration'].value_counts().head(10)\n",
        "\n",
        "missing_year_of_registration = cars['year_of_registration'].isnull().sum()\n",
        "\n",
        "print(\"Basic Statistics:\")\n",
        "print(year_of_registration_stats)\n",
        "\n",
        "print(\"\\nUnique Values Count:\")\n",
        "print(unique_years)\n",
        "\n",
        "print(\"\\nTop 10 Frequent Years:\")\n",
        "print(year_of_registration_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_year_of_registration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m91C9yS-Mbku"
      },
      "outputs": [],
      "source": [
        "year_data = cars['year_of_registration']\n",
        "year_data = year_data[year_data >= 2000]\n",
        "\n",
        "year_mean = year_data.mean()\n",
        "year_median = year_data.median()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.hist(year_data, bins=range(2000, 2021, 2), edgecolor='black', alpha=0.7, color='skyblue', rwidth=0.85)\n",
        "\n",
        "plt.axvline(year_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {year_mean:.1f}')\n",
        "plt.axvline(year_median, color='green', linestyle='--', linewidth=2, label=f'Median: {year_median:.1f}')\n",
        "\n",
        "plt.text(year_mean, plt.ylim()[1] * 0.9, f'Mean: {int(year_mean)}', color='red', fontsize=12, weight='bold')\n",
        "plt.text(year_median, plt.ylim()[1] * 0.8, f'Median: {int(year_median)}', color='green', fontsize=12, weight='bold')\n",
        "\n",
        "plt.xticks(range(2000, 2021, 5), fontsize=12, rotation=45)\n",
        "plt.xlabel('Year of Registration', fontsize=15, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=15, fontweight='bold')\n",
        "\n",
        "plt.title('Distribution of Year of Registration (2000 and onwards)', fontsize=18, fontweight='bold', color='darkblue')\n",
        "plt.grid(axis='y', alpha=0.5, linestyle='--')\n",
        "\n",
        "plt.legend(fontsize=12, loc='upper left')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_Vlx9-hT5Wh"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'mileage'\n",
        "\n",
        "mileage_stats = cars['mileage'].describe()\n",
        "\n",
        "unique_mileages = cars['mileage'].nunique()\n",
        "\n",
        "mileage_freq = cars['mileage'].value_counts().head(10)\n",
        "\n",
        "missing_mileage = cars['mileage'].isnull().sum()\n",
        "\n",
        "total_cars = len(cars)\n",
        "\n",
        "below_10k = cars[cars['mileage'] < 10000].shape[0]\n",
        "percentage_below_10k = (below_10k / total_cars) * 100\n",
        "\n",
        "between_10k_100k = cars[(cars['mileage'] >= 10000) & (cars['mileage'] <= 100000)].shape[0]\n",
        "percentage_between_10k_100k = (between_10k_100k / total_cars) * 100\n",
        "\n",
        "above_100k = cars[cars['mileage'] > 100000].shape[0]\n",
        "percentage_above_100k = (above_100k / total_cars) * 100\n",
        "\n",
        "print(\"Basic Statistics:\")\n",
        "print(mileage_stats)\n",
        "\n",
        "# print(\"\\nUnique Values Count:\")\n",
        "# print(unique_mileages)\n",
        "\n",
        "# print(\"\\nTop 10 Frequent Mileages:\")\n",
        "# print(mileage_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_mileage)\n",
        "\n",
        "print(\"\\nCars with mileage < 10,000:\")\n",
        "print(f\"{percentage_below_10k:.2f}%\")\n",
        "\n",
        "print(\"\\nCars with mileage between 10,000 and 100,000:\")\n",
        "print(f\"{percentage_between_10k_100k:.2f}%\")\n",
        "\n",
        "print(\"\\nCars with mileage > 100,000:\")\n",
        "print(f\"{percentage_above_100k:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj1YiuzMizuM"
      },
      "outputs": [],
      "source": [
        "mileage_data = pd.to_numeric(cars['mileage'], errors='coerce')\n",
        "mileage_data = mileage_data[mileage_data > 0]\n",
        "\n",
        "log_mileage_data = mileage_data.apply(np.log)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.hist(log_mileage_data.dropna(), bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "\n",
        "mean_log = log_mileage_data.mean()\n",
        "median_log = log_mileage_data.median()\n",
        "plt.axvline(mean_log, color='red', linestyle='--', linewidth=2, label=f'Mean (Log): {mean_log:.2f}')\n",
        "plt.axvline(median_log, color='green', linestyle='--', linewidth=2, label=f'Median (Log): {median_log:.2f}')\n",
        "\n",
        "log_ticks = [np.log(x) for x in [1, 100, 1000, 10000, 30000, 50000, 100000, 200000, 300000, 500000, 1000000]]\n",
        "tick_labels = ['1', '100', '1000', '10000', '30000', '50000', '100000', '200000', '300000', '500000', '1000000']\n",
        "plt.xticks(log_ticks, tick_labels, fontsize=12, rotation=90)\n",
        "plt.xlabel('Mileage (Miles)', fontsize=15, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=15, fontweight='bold')\n",
        "\n",
        "plt.title('Histogram of Mileage (Log Scale)', fontsize=18, fontweight='bold', color='darkblue')\n",
        "plt.grid(axis='y', alpha=0.5, linestyle='--')\n",
        "\n",
        "plt.text(mean_log, plt.ylim()[1] * 0.9, f'Mean: ~{int(np.exp(mean_log))} miles', color='red', fontsize=12, weight='bold')\n",
        "plt.text(median_log, plt.ylim()[1] * 0.8, f'Median: ~{int(np.exp(median_log))} miles', color='green', fontsize=12, weight='bold')\n",
        "\n",
        "plt.legend(fontsize=12, loc='upper left')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVGY6cCJbJdC"
      },
      "outputs": [],
      "source": [
        "sqrt_mileage = np.sqrt(cars['mileage'].dropna())\n",
        "\n",
        "mean_mileage = cars['mileage'].mean()\n",
        "median_mileage = cars['mileage'].median()\n",
        "\n",
        "mean_sqrt = np.sqrt(mean_mileage)\n",
        "median_sqrt = np.sqrt(median_mileage)\n",
        "\n",
        "adjusted_rounded_ticks = [0, 1000, 10000, 30000, 50000, 100000, 200000, 300000, 500000, 1000000]\n",
        "sqrt_adjusted_ticks = np.sqrt(adjusted_rounded_ticks)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(sqrt_mileage, bins=30, edgecolor='black', alpha=0.7, color='orange', label='Mileage Distribution')\n",
        "\n",
        "plt.axvline(mean_sqrt, color='red', linestyle='--', linewidth=1.5, label=f'Mean ({int(mean_mileage):,})')\n",
        "plt.axvline(median_sqrt, color='blue', linestyle='--', linewidth=1.5, label=f'Median ({int(median_mileage):,})')\n",
        "\n",
        "plt.xticks(sqrt_adjusted_ticks, labels=[f\"{int(val):,}\" for val in adjusted_rounded_ticks], rotation=90)\n",
        "\n",
        "plt.title('Distribution of Mileage with Mean and Median', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Mileage (Rounded Values)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4eV2IKpRHPL1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.boxplot(\n",
        "    mileage_data.dropna(),\n",
        "    vert=False,\n",
        "    patch_artist=True,\n",
        "    boxprops=dict(facecolor=\"lightblue\", edgecolor=\"black\", linewidth=1.5),\n",
        "    medianprops=dict(color=\"red\", linewidth=2),\n",
        "    whiskerprops=dict(color=\"black\", linewidth=1.5),\n",
        "    capprops=dict(color=\"black\", linewidth=1.5),\n",
        "    flierprops=dict(marker=\"o\", color=\"orange\", markersize=5)\n",
        ")\n",
        "\n",
        "correct_labels = ['0-50k', '50k-100k', '100k-150k', '150k-200k', '200k+']\n",
        "correct_ticks = [25000, 75000, 125000, 175000, 225000]\n",
        "\n",
        "plt.title('Box Plot of Mileage Categorized by Ranges', fontsize=18, fontweight='bold', color='darkblue')\n",
        "plt.xlabel('Mileage Ranges (Miles)', fontsize=15, fontweight='bold')\n",
        "\n",
        "plt.xticks(correct_ticks, correct_labels, fontsize=12, rotation=45)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9T2iugL5CbEx"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'price'\n",
        "\n",
        "price_stats = cars['price'].describe()\n",
        "\n",
        "unique_prices = cars['price'].nunique()\n",
        "\n",
        "price_freq = cars['price'].value_counts().head(10)\n",
        "\n",
        "missing_price = cars['price'].isnull().sum()\n",
        "\n",
        "print(\"Basic Statistics:\")\n",
        "print(price_stats)\n",
        "\n",
        "print(\"\\nUnique Values Count:\")\n",
        "print(unique_prices)\n",
        "\n",
        "print(\"\\nTop 10 Frequent Prices:\")\n",
        "print(price_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wyE3j2bENJMO"
      },
      "outputs": [],
      "source": [
        "mean_price = cars['price'].mean()\n",
        "median_price = cars['price'].median()\n",
        "\n",
        "def format_func(value, tick_number):\n",
        "    return f'{int(value):,}'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.hist(cars['price'].dropna(), bins=100, edgecolor='black', alpha=0.7, label='Histogram')\n",
        "\n",
        "plt.axvline(mean_price, color='red', linestyle='--', label=f'Mean: {int(mean_price):,}')\n",
        "plt.axvline(median_price, color='green', linestyle='--', label=f'Median: {int(median_price):,}')\n",
        "\n",
        "plt.title('Histogram of Price (Log Scale, 0 to 10,000,000)', fontsize=14, color='blue')\n",
        "plt.xlabel('Price', fontsize=12)\n",
        "plt.ylabel('Frequency (Log Scale)', fontsize=12)\n",
        "\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(format_func))\n",
        "\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eniDLpH4B5Fw"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'vehicle_condition'\n",
        "\n",
        "unique_conditions = cars['vehicle_condition'].nunique()\n",
        "\n",
        "condition_freq = cars['vehicle_condition'].value_counts()\n",
        "\n",
        "missing_vehicle_condition = cars['vehicle_condition'].isnull().sum()\n",
        "\n",
        "print(\"Unique Values Count:\")\n",
        "print(unique_conditions)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(condition_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_vehicle_condition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nQ2HZeVNPUgE"
      },
      "outputs": [],
      "source": [
        "vehicle_condition_counts = cars['vehicle_condition'].value_counts(normalize=True) * 100\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "vehicle_condition_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=['skyblue', 'orange'])\n",
        "plt.title(\"Vehicle Condition Distribution\")\n",
        "plt.ylabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "72B1-mhyPgIN"
      },
      "outputs": [],
      "source": [
        "vehicle_condition_counts = cars['vehicle_condition'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(vehicle_condition_counts.index, vehicle_condition_counts.values, color=['skyblue', 'orange'])\n",
        "plt.title(\"Vehicle Condition Distribution\", fontsize=14)\n",
        "plt.xlabel(\"Vehicle Condition\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.xticks(rotation=0, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 5000,\n",
        "             str(int(bar.get_height())), ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Finh4U0zCuAH"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'body_type'\n",
        "\n",
        "unique_body_types = cars['body_type'].nunique()\n",
        "\n",
        "body_type_freq = cars['body_type'].value_counts()\n",
        "\n",
        "missing_body_type = cars['body_type'].isnull().sum()\n",
        "\n",
        "print(\"Unique Values Count:\")\n",
        "print(unique_body_types)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(body_type_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_body_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rOv8e1x8H2AM"
      },
      "outputs": [],
      "source": [
        "body_type_data = cars['body_type']\n",
        "\n",
        "body_type_counts = body_type_data.value_counts(dropna=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "body_type_counts.plot(kind='bar', color='skyblue', edgecolor='black', alpha=0.8)\n",
        "plt.title('Distribution of Body Types (Including Missing Values)', fontsize=18, fontweight='bold', color='darkblue')\n",
        "plt.xlabel('Body Type', fontsize=15, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=15, fontweight='bold')\n",
        "plt.xticks(rotation=45, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "for index, value in enumerate(body_type_counts):\n",
        "    plt.text(index, value + 1000, str(value), ha='center', fontsize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxIhUEMogYlC"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'fuel_type'\n",
        "\n",
        "unique_fuel_types = cars['fuel_type'].nunique()\n",
        "\n",
        "fuel_type_freq = cars['fuel_type'].value_counts()\n",
        "fuel_type_percentage = (cars['fuel_type'].value_counts(normalize=True) * 100).round(2)\n",
        "\n",
        "missing_fuel_type = cars['fuel_type'].isnull().sum()\n",
        "\n",
        "print(\"Unique Values Count:\")\n",
        "print(unique_fuel_types)\n",
        "\n",
        "print(\"\\nFrequency Distribution (Count and Percentage):\")\n",
        "for fuel, count in fuel_type_freq.items():\n",
        "    print(f\"{fuel}: {count} ({fuel_type_percentage[fuel]}%)\")\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_fuel_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YGLFbpUwHl-I"
      },
      "outputs": [],
      "source": [
        "fuel_type_data = cars['fuel_type']\n",
        "\n",
        "fuel_type_counts = fuel_type_data.value_counts(dropna=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "fuel_type_counts.plot(kind='bar', color='green', edgecolor='black', alpha=0.8)\n",
        "plt.title('Distribution of Fuel Types (Including Missing Values)', fontsize=18, fontweight='bold', color='darkblue')\n",
        "plt.xlabel('Fuel Type', fontsize=15, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=15, fontweight='bold')\n",
        "plt.xticks(rotation=45, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "for index, value in enumerate(fuel_type_counts):\n",
        "    plt.text(index, value + 1000, str(value), ha='center', fontsize=10)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Yl5Jo1woDXEI"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'crossover_car_and_van'\n",
        "\n",
        "unique_crossover_values = cars['crossover_car_and_van'].nunique()\n",
        "\n",
        "crossover_freq = cars['crossover_car_and_van'].value_counts()\n",
        "\n",
        "missing_crossover = cars['crossover_car_and_van'].isnull().sum()\n",
        "\n",
        "print(\"Unique Values Count:\")\n",
        "print(unique_crossover_values)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(crossover_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_crossover)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1GUgldTxTDPO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "cars['crossover_car_and_van'].value_counts().plot(\n",
        "    kind='pie', autopct='%1.1f%%', colors=['skyblue', 'orange'],\n",
        "    labels=['False', 'True'], startangle=90, explode=[0, 0.1]\n",
        ")\n",
        "\n",
        "plt.title('Distribution of Crossover Car and Van', fontsize=14, color='blue')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TpgQ17HJQQpD"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "cars['crossover_car_and_van'].value_counts().plot(\n",
        "    kind='bar', color=['skyblue', 'orange'], edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title('Distribution of Crossover Car and Van', fontsize=14, color='blue')\n",
        "plt.xlabel('Crossover Car and Van', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "for index, value in enumerate(cars['crossover_car_and_van'].value_counts()):\n",
        "    plt.text(index, value + 5000, f'{value:,}', ha='center', fontsize=10)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks([0, 1], ['False', 'True'], rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lMVvozmBDXAR"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'standard_make'\n",
        "\n",
        "unique_makes = cars['standard_make'].nunique()\n",
        "\n",
        "make_freq = cars['standard_make'].value_counts().head(10)\n",
        "\n",
        "missing_standard_make = cars['standard_make'].isnull().sum()\n",
        "\n",
        "print(\"Unique Values Count:\")\n",
        "print(unique_makes)\n",
        "\n",
        "print(\"\\nTop 10 Frequent Makes:\")\n",
        "print(make_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_standard_make)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6rcecSYNRPdK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "cars['standard_make'].value_counts().head(10).plot(\n",
        "    kind='bar', color='skyblue', edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title('Top 10 Most Frequent standard_Makes', fontsize=14, color='blue')\n",
        "plt.xlabel('Car Make', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "for index, value in enumerate(cars['standard_make'].value_counts().head(10)):\n",
        "    plt.text(index, value + 1000, f'{value:,}', ha='center', fontsize=10)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QWZOdLmaENPN"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'standard_model'\n",
        "\n",
        "unique_models = cars['standard_model'].nunique()\n",
        "\n",
        "model_freq = cars['standard_model'].value_counts().head(10)\n",
        "\n",
        "missing_standard_model = cars['standard_model'].isnull().sum()\n",
        "\n",
        "print(\"Unique Values Count:\")\n",
        "print(unique_models)\n",
        "\n",
        "print(\"\\nTop 10 Frequent Models:\")\n",
        "print(model_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_standard_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xmul1WGlR1KS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "cars['standard_model'].value_counts().head(20).plot(\n",
        "    kind='bar', color='lightcoral', edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title('Top 20 Most Frequent Car Models', fontsize=14, color='blue')\n",
        "plt.xlabel('Car Model', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "for index, value in enumerate(cars['standard_model'].value_counts().head(20)):\n",
        "    plt.text(index, value + 300, f'{value:,}', ha='center', fontsize=9)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p8Ogtu4mDWwa"
      },
      "outputs": [],
      "source": [
        "# Analysis for 'standard_colour'\n",
        "\n",
        "unique_colours = cars['standard_colour'].nunique()\n",
        "\n",
        "colour_freq = cars['standard_colour'].value_counts().head(10)\n",
        "\n",
        "missing_standard_colour = cars['standard_colour'].isnull().sum()\n",
        "\n",
        "print(\"Unique Values Count:\")\n",
        "print(unique_colours)\n",
        "\n",
        "print(\"\\nTop 10 Frequent Colours:\")\n",
        "print(colour_freq)\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(missing_standard_colour)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BtcCKFzHR8GV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "cars['standard_colour'].value_counts().plot(\n",
        "    kind='bar', color='mediumpurple', edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title('Distribution of Standard Colours', fontsize=14, color='blue')\n",
        "plt.xlabel('Colour', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "for index, value in enumerate(cars['standard_colour'].value_counts()):\n",
        "        plt.text(index, value + 1000, f'{value:,}', ha='center', fontsize=10)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez6qjJgV2wid"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07sjk7nRji95"
      },
      "outputs": [],
      "source": [
        "correlations = cars[['mileage', 'year_of_registration', 'price']].corr()\n",
        "print(correlations)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(correlations, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SSXEMpPniiR"
      },
      "outputs": [],
      "source": [
        "numeric_columns = ['mileage', 'year_of_registration']\n",
        "\n",
        "numeric_correlations = cars[numeric_columns].corrwith(cars['price'])\n",
        "print(numeric_correlations)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(numeric_correlations.index, numeric_correlations.values, color=['skyblue', 'orange'])\n",
        "plt.title(\"Correlation of 'mileage' and 'year_of_registration' with 'price'\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Features\", fontsize=12)\n",
        "plt.ylabel(\"Correlation with Price\", fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s9TVvs4kz9u"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "categorical_columns = cars.select_dtypes(include=['object', 'bool', 'category']).columns\n",
        "\n",
        "anova_scores = {}\n",
        "for col in categorical_columns:\n",
        "    if cars[col].nunique() > 1:\n",
        "        groups = [cars[cars[col] == category]['price'] for category in cars[col].unique()]\n",
        "        f_stat, p_value = f_oneway(*groups)\n",
        "        anova_scores[col] = f_stat\n",
        "\n",
        "anova_scores = pd.Series(anova_scores).sort_values(ascending=False)\n",
        "\n",
        "print(\"ANOVA F-Statistics for Categorical Features with Price:\")\n",
        "print(anova_scores)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(anova_scores.index, anova_scores.values, color='green')\n",
        "plt.title(\"ANOVA F-Statistics for Categorical Features with Price\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Features\", fontsize=12)\n",
        "plt.ylabel(\"F-Statistic\", fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOuNlDKK0hZL"
      },
      "outputs": [],
      "source": [
        "price_min = 0\n",
        "price_max = 50000\n",
        "\n",
        "top_10_models = cars['standard_model'].value_counts().head(10).index\n",
        "\n",
        "filtered_data_limited = cars[\n",
        "    (cars['standard_model'].isin(top_10_models)) &\n",
        "    (cars['price'] >= price_min) &\n",
        "    (cars['price'] <= price_max)\n",
        "]\n",
        "\n",
        "custom_palette = {True: \"#1f77b4\", False: \"#ff7f0e\"}\n",
        "\n",
        "sns.boxplot(\n",
        "    x=filtered_data_limited['standard_model'],\n",
        "    y=filtered_data_limited['price'],\n",
        "    hue=filtered_data_limited['year_of_registration'] > 2010,\n",
        "    palette=custom_palette,\n",
        "    showmeans=False,\n",
        "    meanline=False,\n",
        "    flierprops={'marker': 'o', 'markersize': 5, 'color': 'black'},\n",
        "    whiskerprops={'linewidth': 1},\n",
        "    boxprops={'linewidth': 1}\n",
        ")\n",
        "\n",
        "plt.title(f\"Price Distribution by 10 Most Frequent Models (Price Range: {price_min:,} - {price_max:,})\")\n",
        "plt.xlabel(\"Standard Model\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend(title=\"Year > 2010\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76wHnVai1C3S"
      },
      "outputs": [],
      "source": [
        "categorical_columns = cars.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "category_price_relationships = {}\n",
        "for column in categorical_columns:\n",
        "    mean_price = cars.groupby(column)['price'].mean()\n",
        "    category_price_relationships[column] = mean_price\n",
        "\n",
        "for column, values in category_price_relationships.items():\n",
        "    print(f\"Column: {column}\")\n",
        "    print(values)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YJgIhAUCjizj"
      },
      "outputs": [],
      "source": [
        "qualitative_cols = ['vehicle_condition', 'body_type', 'fuel_type', 'crossover_car_and_van',\n",
        "                    'standard_make', 'standard_model', 'standard_colour']\n",
        "\n",
        "for col in qualitative_cols:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    if col in ['standard_make', 'standard_model']:\n",
        "        mean_prices = cars.groupby(col)['price'].mean().sort_values(ascending=False).head(20)\n",
        "        title_suffix = \" (Top 20)\"\n",
        "    else:\n",
        "        mean_prices = cars.groupby(col)['price'].mean().sort_values(ascending=False)\n",
        "        title_suffix = \"\"\n",
        "\n",
        "    sns.barplot(x=mean_prices.index, y=mean_prices.values, palette='viridis')\n",
        "    plt.title(f\"Average Price by {col}{title_suffix}\", fontsize=16, fontweight='bold')\n",
        "    plt.xlabel(col.capitalize(), fontsize=12)\n",
        "    plt.ylabel(\"Average Price\", fontsize=12)\n",
        "    plt.xticks(rotation=90, fontsize=10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nzi5jWQnomk"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muUHgUimGq6E"
      },
      "outputs": [],
      "source": [
        "summary_table = pd.DataFrame({\n",
        "    'Missing Values': cars.isnull().sum(),\n",
        "    'Unique Values': cars.nunique(),\n",
        "    'Data Type': cars.dtypes,\n",
        "    'Mean': cars.select_dtypes(include='number').mean(),\n",
        "    'Median': cars.select_dtypes(include='number').median(),\n",
        "    'Min': cars.select_dtypes(include='number').min(),\n",
        "    'Max': cars.select_dtypes(include='number').max()\n",
        "}).reset_index()\n",
        "\n",
        "summary_table.rename(columns={'index': 'Column'}, inplace=True)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(summary_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txhYmJlmw7QE"
      },
      "outputs": [],
      "source": [
        "missing_count = cars['year_of_registration'].isnull().sum()\n",
        "total_count = len(cars)\n",
        "missing_percentage = (missing_count / total_count) * 100\n",
        "print(f\"Missing values of year_of_registration: {missing_count} ({missing_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPaDJIK0xTdp"
      },
      "outputs": [],
      "source": [
        "print(cars['year_of_registration'].min(), cars['year_of_registration'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JHGKjI_dxbbR"
      },
      "outputs": [],
      "source": [
        "outliers_before_1900 = cars[cars['year_of_registration'] < 1900]\n",
        "\n",
        "print(f\"Number of records with 'year_of_registration' before 1900: {len(outliers_before_1900)}\")\n",
        "outliers_before_1900.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViMR3oTIzaSR"
      },
      "outputs": [],
      "source": [
        "new_vehicles_mileage = cars[cars['vehicle_condition'] == 'NEW']['mileage']\n",
        "\n",
        "print(new_vehicles_mileage.min())\n",
        "print(new_vehicles_mileage.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpv1y9ppzeNk"
      },
      "outputs": [],
      "source": [
        "print(cars['year_of_registration'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-MKbS_--zlDI"
      },
      "outputs": [],
      "source": [
        "missing_year_with_reg_code = cars[cars['reg_code'].notna() & cars['year_of_registration'].isna()]\n",
        "\n",
        "missing_year_with_reg_code.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBhpJF-Pz4Cq"
      },
      "outputs": [],
      "source": [
        "missing_both = cars[cars['year_of_registration'].isna() & cars['reg_code'].isna()]\n",
        "\n",
        "missing_both_count = missing_both.shape[0]\n",
        "missing_both_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSLiHocm0Gbk"
      },
      "outputs": [],
      "source": [
        "vehicles_with_low_mileage_and_no_year = cars[\n",
        "    (cars['mileage'] >= 0) &\n",
        "    (cars['mileage'] <= 100) &\n",
        "    (cars['year_of_registration'].isnull())\n",
        "]\n",
        "\n",
        "print(f\"Number of vehicles: {vehicles_with_low_mileage_and_no_year.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZaKHa4EZiw1"
      },
      "outputs": [],
      "source": [
        "# filling missing values of year_of_registration for New Cars\n",
        "import datetime\n",
        "\n",
        "current_year = datetime.datetime.now().year\n",
        "\n",
        "max_year = cars['year_of_registration'].max()\n",
        "if max_year + 1 <= current_year:\n",
        "    fill_year = max_year + 1\n",
        "else:\n",
        "    fill_year = current_year\n",
        "\n",
        "cars.loc[(cars['vehicle_condition'] == 'NEW') & (cars['mileage'].between(0, 100)), 'year_of_registration'] = fill_year\n",
        "cars['year_of_registration'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyXlTMzxaC50"
      },
      "outputs": [],
      "source": [
        "new_cars_years = cars.loc[cars['vehicle_condition'] == 'NEW', 'year_of_registration'].unique()\n",
        "\n",
        "print(new_cars_years)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HitjNDb10GZg"
      },
      "outputs": [],
      "source": [
        "# filling missing values of year_of_registration after 2001\n",
        "\n",
        "valid_reg_code = cars['reg_code'].str.isdigit() & (cars['reg_code'].str.len() == 2)\n",
        "\n",
        "cars.loc[valid_reg_code, 'reg_code_numeric'] = cars.loc[valid_reg_code, 'reg_code'].astype(int)\n",
        "\n",
        "calculated_year_1_to_50 = 2000 + cars.loc[valid_reg_code &\n",
        "                                          (cars['reg_code_numeric'] >= 1) & (cars['reg_code_numeric'] <= 50),\n",
        "                                          'reg_code_numeric']\n",
        "\n",
        "cars.loc[\n",
        "    valid_reg_code & (cars['reg_code_numeric'] >= 1) & (cars['reg_code_numeric'] <= 50) &\n",
        "    (calculated_year_1_to_50 <= 2021), 'year_of_registration'\n",
        "] = calculated_year_1_to_50\n",
        "\n",
        "calculated_year_51_to_99 = 2000 + cars.loc[valid_reg_code &\n",
        "                                          (cars['reg_code_numeric'] >= 51) & (cars['reg_code_numeric'] <= 99),\n",
        "                                          'reg_code_numeric'] - 50\n",
        "\n",
        "cars.loc[\n",
        "    valid_reg_code & (cars['reg_code_numeric'] >= 51) & (cars['reg_code_numeric'] <= 99) &\n",
        "    (calculated_year_51_to_99 <= 2021), 'year_of_registration'\n",
        "] = calculated_year_51_to_99\n",
        "\n",
        "cars.drop(columns=['reg_code_numeric'], inplace=True)\n",
        "cars['year_of_registration'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgMHDZfz9wza"
      },
      "outputs": [],
      "source": [
        "# fill missing values of year_of_registration between 1963 and 2001\n",
        "\n",
        "year_from_reg_code = {\n",
        "    1963: 'A', 1964: 'B', 1965: 'C', 1966: 'D', 1967: ['E', 'F'], 1968: 'F', 1969: 'G', 1970: 'H',\n",
        "    1971: 'J', 1972: 'K', 1973: 'L', 1974: 'M', 1975: 'N', 1976: 'P', 1977: 'R', 1978: 'S',\n",
        "    1979: 'T', 1980: 'V', 1981: 'W', 1982: 'X', 1983: 'A', 1984: 'B', 1985: 'C', 1986: 'D',\n",
        "    1987: 'E', 1988: 'F', 1989: 'G', 1990: 'H', 1991: 'J', 1992: 'K', 1993: 'L', 1994: 'M',\n",
        "    1995: 'N', 1996: 'P', 1997: 'R', 1998: 'S', 1999: ['T', 'V'], 2000: ['W', 'X'], 2001: 'Y'\n",
        "}\n",
        "\n",
        "reg_code_to_year = {}\n",
        "for year, codes in year_from_reg_code.items():\n",
        "    if isinstance(codes, list):\n",
        "        for code in codes:\n",
        "            reg_code_to_year[code] = year\n",
        "    else:\n",
        "        reg_code_to_year[codes] = year\n",
        "\n",
        "for index, row in cars.iterrows():\n",
        "    if (pd.isnull(row['year_of_registration']) or row['year_of_registration'] == 0) and pd.notnull(row['reg_code']):\n",
        "\n",
        "        cars.at[index, 'year_of_registration'] = reg_code_to_year.get(row['reg_code'], None)\n",
        "    elif pd.isnull(row['reg_code']) and pd.notnull(row['year_of_registration']):\n",
        "        year = int(row['year_of_registration'])\n",
        "        reg_code_options = year_from_reg_code.get(year, None)\n",
        "        if isinstance(reg_code_options, list):\n",
        "            cars.at[index, 'reg_code'] = random.choice(reg_code_options)\n",
        "        else:\n",
        "            cars.at[index, 'reg_code'] = reg_code_options\n",
        "cars['year_of_registration'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALVf1axD5ipY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9hRAKe-XrDf"
      },
      "outputs": [],
      "source": [
        "missing_count = cars['mileage'].isnull().sum()\n",
        "total_count = len(cars)\n",
        "missing_percentage = (missing_count / total_count) * 100\n",
        "print(f\"Missing values: {missing_count} ({missing_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhNpkDKlYCyf"
      },
      "outputs": [],
      "source": [
        "mileage_stats = cars['mileage'].describe()\n",
        "\n",
        "print(\"Descriptive Statistics for Mileage:\")\n",
        "print(mileage_stats)\n",
        "\n",
        "Q1 = mileage_stats['25%']\n",
        "Q3 = mileage_stats['75%']\n",
        "IQR = Q3 - Q1\n",
        "print(f\"\\nInterquartile Range (IQR): {IQR}\")\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "outliers = cars[(cars['mileage'] < lower_bound) | (cars['mileage'] > upper_bound)]\n",
        "print(f\"\\nNumber of Outliers: {len(outliers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gIxUG1lBUv3"
      },
      "outputs": [],
      "source": [
        "correlation_with_year = cars['mileage'].corr(cars['year_of_registration'])\n",
        "correlation_with_price = cars['mileage'].corr(cars['price'])\n",
        "\n",
        "print(f\"Correlation between mileage and year_of_registration: {correlation_with_year}\")\n",
        "print(f\"Correlation between mileage and price: {correlation_with_price}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YrYMYKyBZr1"
      },
      "outputs": [],
      "source": [
        "qualitative_columns = cars.select_dtypes(include=['object', 'bool']).columns\n",
        "\n",
        "relationship_strength_mileage = {}\n",
        "for column in qualitative_columns:\n",
        "    if cars[column].nunique() > 1:\n",
        "        category_means = cars.groupby(column)['mileage'].mean()\n",
        "        overall_mean = cars['mileage'].mean()\n",
        "        relationship_strength_mileage[column] = ((category_means - overall_mean) ** 2).sum()\n",
        "\n",
        "relationship_strength_mileage_sorted = sorted(relationship_strength_mileage.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for column, strength in relationship_strength_mileage_sorted:\n",
        "    print(f\"Column: {column}, Relationship Strength: {strength}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxT3TVAQm8co"
      },
      "outputs": [],
      "source": [
        "grouped_mileage_make_model_year = cars.groupby(\n",
        "    ['standard_make', 'standard_model', 'year_of_registration']\n",
        ")['mileage'].median()\n",
        "\n",
        "grouped_mileage_make_model_year = grouped_mileage_make_model_year.to_dict()\n",
        "\n",
        "def estimate_mileage_make_model_year(row):\n",
        "    if pd.isnull(row['mileage']):\n",
        "        return grouped_mileage_make_model_year.get(\n",
        "            (row['standard_make'], row['standard_model'], row['year_of_registration']), None\n",
        "        )\n",
        "    return row['mileage']\n",
        "\n",
        "cars['mileage'] = cars.apply(estimate_mileage_make_model_year, axis=1)\n",
        "\n",
        "remaining_missing_mileage = cars['mileage'].isnull().sum()\n",
        "print(f\"Remaining missing mileage values after considering make, model, and year: {remaining_missing_mileage}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXYhUDRXQUGD"
      },
      "outputs": [],
      "source": [
        "grouped_mileage_make_model = cars.groupby(\n",
        "    ['standard_make', 'standard_model']\n",
        ")['mileage'].median()\n",
        "\n",
        "grouped_mileage_make_model = grouped_mileage_make_model.to_dict()\n",
        "\n",
        "def estimate_mileage_make_model(row):\n",
        "    if pd.isnull(row['mileage']):\n",
        "        return grouped_mileage_make_model.get(\n",
        "            (row['standard_make'], row['standard_model']), None\n",
        "        )\n",
        "    return row['mileage']\n",
        "\n",
        "cars['mileage'] = cars.apply(estimate_mileage_make_model, axis=1)\n",
        "\n",
        "remaining_missing_mileage = cars['mileage'].isnull().sum()\n",
        "print(f\"Remaining missing mileage values after considering make and model: {remaining_missing_mileage}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXGvANfMbhil"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-CL8blcZxRs"
      },
      "outputs": [],
      "source": [
        "print(cars['body_type'].isnull().sum())\n",
        "null_percentage = (cars['body_type'].isnull().sum() / len(cars)) * 100\n",
        "print(f\"Percentage of missing body_type: {null_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNa9YSe-Z9Ej"
      },
      "outputs": [],
      "source": [
        "print(cars['body_type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAXjrnm5aH7X"
      },
      "outputs": [],
      "source": [
        "numerical_columns = cars.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "relationship_strength_numeric = {}\n",
        "for column in numerical_columns:\n",
        "    if cars[column].notnull().sum() > 0:\n",
        "        category_means = cars.groupby('body_type')[column].mean()\n",
        "        overall_mean = cars[column].mean()\n",
        "        relationship_strength_numeric[column] = ((category_means - overall_mean) ** 2).sum()\n",
        "\n",
        "relationship_strength_numeric_sorted = sorted(relationship_strength_numeric.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for column, strength in relationship_strength_numeric_sorted:\n",
        "    print(f\"Column: {column}, Relationship Strength: {strength}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNsVtHSpa5rL"
      },
      "outputs": [],
      "source": [
        "qualitative_columns = cars.select_dtypes(include=['object', 'bool']).columns\n",
        "\n",
        "relationship_strength_body_type = {}\n",
        "for column in qualitative_columns:\n",
        "    if cars[column].nunique() > 1:\n",
        "        category_means = cars.groupby(column)['body_type'].apply(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "        overall_mode = cars['body_type'].mode()[0]\n",
        "        relationship_strength_body_type[column] = ((category_means != overall_mode).sum())\n",
        "\n",
        "relationship_strength_body_type_sorted = sorted(relationship_strength_body_type.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for column, strength in relationship_strength_body_type_sorted:\n",
        "    print(f\"Column: {column}, Relationship Strength: {strength}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yx7XCPt2lp1"
      },
      "outputs": [],
      "source": [
        "grouped_by_features = cars.groupby(\n",
        "    ['standard_make', 'standard_model', 'price_bins', 'mileage_bins', 'crossover_car_and_van']\n",
        ")['body_type'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "\n",
        "def estimate_body_type_from_grouped_features(row):\n",
        "    if pd.isnull(row['body_type']):\n",
        "        return grouped_by_features.get(\n",
        "            (row['standard_make'], row['standard_model'], row['price_bins'], row['mileage_bins'], row['crossover_car_and_van']),\n",
        "            None\n",
        "        )\n",
        "    return row['body_type']\n",
        "\n",
        "cars['body_type'] = cars.apply(estimate_body_type_from_grouped_features, axis=1)\n",
        "\n",
        "remaining_missing = cars['body_type'].isnull().sum()\n",
        "print(f\"Remaining missing body_type values: {remaining_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeTgv5HF2_Ks"
      },
      "outputs": [],
      "source": [
        "grouped_by_features = cars.groupby(\n",
        "    ['standard_model', 'price_bins', 'mileage_bins']\n",
        ")['body_type'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "\n",
        "def estimate_body_type_from_grouped_features(row):\n",
        "    if pd.isnull(row['body_type']):\n",
        "        return grouped_by_features.get(\n",
        "            (row['standard_model'], row['price_bins'], row['mileage_bins']),\n",
        "            None\n",
        "        )\n",
        "    return row['body_type']\n",
        "\n",
        "cars['body_type'] = cars.apply(estimate_body_type_from_grouped_features, axis=1)\n",
        "\n",
        "remaining_missing = cars['body_type'].isnull().sum()\n",
        "print(f\"Remaining missing body_type values: {remaining_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kFYZC_M3Ks7"
      },
      "outputs": [],
      "source": [
        "grouped_by_features = cars.groupby(\n",
        "    ['standard_make', 'price_bins', 'mileage_bins']\n",
        ")['body_type'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "\n",
        "def estimate_body_type_from_grouped_features(row):\n",
        "    if pd.isnull(row['body_type']):\n",
        "        return grouped_by_features.get(\n",
        "            (row['standard_make'], row['price_bins'], row['mileage_bins']),\n",
        "            None\n",
        "        )\n",
        "    return row['body_type']\n",
        "\n",
        "cars['body_type'] = cars.apply(estimate_body_type_from_grouped_features, axis=1)\n",
        "\n",
        "remaining_missing = cars['body_type'].isnull().sum()\n",
        "print(f\"Remaining missing body_type values: {remaining_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3M4WJsWqt2q"
      },
      "outputs": [],
      "source": [
        "grouped_body_type_model = cars.groupby(\n",
        "    ['standard_model']\n",
        ")['body_type'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "\n",
        "def estimate_body_type_model(row):\n",
        "    if pd.isnull(row['body_type']):\n",
        "        return grouped_body_type_model.get(\n",
        "            row['standard_model'], None\n",
        "        )\n",
        "    return row['body_type']\n",
        "\n",
        "cars['body_type'] = cars.apply(estimate_body_type_model, axis=1)\n",
        "\n",
        "remaining_missing_body_type = cars['body_type'].isnull().sum()\n",
        "print(f\"Remaining missing body_type values after using model: {remaining_missing_body_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLjsX101rDK_"
      },
      "outputs": [],
      "source": [
        "grouped_body_type_make = cars.groupby(\n",
        "    ['standard_make']\n",
        ")['body_type'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "\n",
        "def estimate_body_type_make(row):\n",
        "    if pd.isnull(row['body_type']):\n",
        "        return grouped_body_type_make.get(\n",
        "            row['standard_make'], None\n",
        "        )\n",
        "    return row['body_type']\n",
        "\n",
        "cars['body_type'] = cars.apply(estimate_body_type_make, axis=1)\n",
        "\n",
        "remaining_missing_body_type = cars['body_type'].isnull().sum()\n",
        "print(f\"Remaining missing body_type values after using make: {remaining_missing_body_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky1NlkP2rwZF"
      },
      "outputs": [],
      "source": [
        "missing_body_type_records = cars[cars['body_type'].isnull()]\n",
        "\n",
        "print(missing_body_type_records)\n",
        "\n",
        "missing_body_type_records.to_csv('missing_body_type_records.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL5zTYnb3V-3"
      },
      "outputs": [],
      "source": [
        "grouped_by_features = cars.groupby(\n",
        "    ['mileage_bins', 'year_of_registration', 'price_bins']\n",
        ")['body_type'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "\n",
        "def estimate_body_type_from_grouped_features(row):\n",
        "    if pd.isnull(row['body_type']):\n",
        "        return grouped_by_features.get(\n",
        "            (row['mileage_bins'], row['year_of_registration'], row['price_bins']),\n",
        "            None\n",
        "        )\n",
        "    return row['body_type']\n",
        "\n",
        "cars['body_type'] = cars.apply(estimate_body_type_from_grouped_features, axis=1)\n",
        "\n",
        "remaining_missing = cars['body_type'].isnull().sum()\n",
        "print(f\"Remaining missing body_type values: {remaining_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfwOrbyZ3fj4"
      },
      "outputs": [],
      "source": [
        "grouped_by_features = cars.groupby(\n",
        "    ['mileage_bins', 'price_bins']\n",
        ")['body_type'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "\n",
        "def estimate_body_type_from_grouped_features(row):\n",
        "    if pd.isnull(row['body_type']):\n",
        "        return grouped_by_features.get(\n",
        "            (row['mileage_bins'], row['price_bins']),\n",
        "            None\n",
        "        )\n",
        "    return row['body_type']\n",
        "\n",
        "cars['body_type'] = cars.apply(estimate_body_type_from_grouped_features, axis=1)\n",
        "\n",
        "remaining_missing = cars['body_type'].isnull().sum()\n",
        "print(f\"Remaining missing body_type values: {remaining_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAR6G0_VgLSl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp2u91LzvCyI"
      },
      "outputs": [],
      "source": [
        "min_mileage = cars['mileage'].min()\n",
        "max_mileage = cars['mileage'].max()\n",
        "\n",
        "print(f\"Minimum Mileage: {min_mileage}\")\n",
        "print(f\"Maximum Mileage: {max_mileage}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiyphtMPvRx6"
      },
      "outputs": [],
      "source": [
        "Q1 = cars['mileage'].quantile(0.25)\n",
        "Q3 = cars['mileage'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = cars[(cars['mileage'] < lower_bound) | (cars['mileage'] > upper_bound)]\n",
        "\n",
        "outliers_count = outliers.shape[0]\n",
        "print(f\"Number of outliers: {outliers_count}\")\n",
        "print(f\"Lower bound: {lower_bound}\")\n",
        "print(f\"Upper bound: {upper_bound}\")\n",
        "\n",
        "print(outliers[['mileage']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umw4xuc2vcWY"
      },
      "outputs": [],
      "source": [
        "high_outliers = cars[cars['mileage'] > upper_bound]\n",
        "\n",
        "print(f\"Number of high outliers: {high_outliers.shape[0]}\")\n",
        "\n",
        "print(high_outliers[['mileage', 'price', 'year_of_registration', 'vehicle_condition']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NY8QyaugXPnY"
      },
      "outputs": [],
      "source": [
        "percentiles = cars['mileage'].dropna().quantile([0.75, 0.9, 0.95, 0.99])\n",
        "\n",
        "def format_ticks(value, _):\n",
        "    return f'{int(value):,}'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(cars['mileage'].dropna(), bins=50, alpha=0.7, label='Mileage Distribution', color='orange')\n",
        "plt.axvline(percentiles[0.75], color='green', linestyle='--', label='75th Percentile')\n",
        "plt.axvline(percentiles[0.9], color='orange', linestyle='--', label='90th Percentile')\n",
        "plt.axvline(percentiles[0.95], color='red', linestyle='--', label='95th Percentile')\n",
        "plt.axvline(percentiles[0.99], color='purple', linestyle='--', label='99th Percentile')\n",
        "\n",
        "plt.title('Mileage Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Mileage', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.legend()\n",
        "\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OATHX0NzHQDn"
      },
      "outputs": [],
      "source": [
        "unique_body_types = cars['body_type'].unique()\n",
        "\n",
        "print(\"Unique values in 'body_type':\")\n",
        "print(unique_body_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8Lrs3aItvxf"
      },
      "outputs": [],
      "source": [
        "sports_types = ['Convertible', 'Coupe']\n",
        "passenger_types = ['SUV', 'Saloon', 'Hatchback', 'Limousine', 'Estate', 'MPV']\n",
        "commercial_types = ['Pickup', 'Combi Van', 'Panel Van', 'Chassis Cab', 'Car Derived Van']\n",
        "special_types = ['Minibus', 'Window Van', 'Camper']\n",
        "\n",
        "def categorize_body_type(body_type):\n",
        "    if body_type in sports_types:\n",
        "        return 'Sports'\n",
        "    elif body_type in passenger_types:\n",
        "        return 'Passenger'\n",
        "    elif body_type in commercial_types:\n",
        "        return 'Commercial'\n",
        "    elif body_type in special_types:\n",
        "        return 'Special'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "cars['vehicle_category'] = cars['body_type'].apply(categorize_body_type)\n",
        "\n",
        "print(cars['vehicle_category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayrKocBFwHB9"
      },
      "outputs": [],
      "source": [
        "cars.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz7NY-3jw81p"
      },
      "outputs": [],
      "source": [
        "def calculate_iqr(data):\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return upper_bound\n",
        "\n",
        "passenger_upper = calculate_iqr(cars[cars['vehicle_category'] == 'Passenger']['mileage'])\n",
        "commercial_upper = calculate_iqr(cars[cars['vehicle_category'] == 'Commercial']['mileage'])\n",
        "special_upper = calculate_iqr(cars[cars['vehicle_category'] == 'Special']['mileage'])\n",
        "sports_upper = calculate_iqr(cars[cars['vehicle_category'] == 'Sports']['mileage'])\n",
        "\n",
        "passenger_percentiles = cars[cars['vehicle_category'] == 'Passenger']['mileage'].quantile([0.95, 0.99])\n",
        "commercial_percentiles = cars[cars['vehicle_category'] == 'Commercial']['mileage'].quantile([0.95, 0.99])\n",
        "special_percentiles = cars[cars['vehicle_category'] == 'Special']['mileage'].quantile([0.95, 0.99])\n",
        "sports_percentiles = cars[cars['vehicle_category'] == 'Sports']['mileage'].quantile([0.95, 0.99])\n",
        "\n",
        "thresholds = {\n",
        "    'Passenger': {\n",
        "        'IQR_Upper': passenger_upper,\n",
        "        '95th Percentile': passenger_percentiles[0.95],\n",
        "        '99th Percentile': passenger_percentiles[0.99]\n",
        "    },\n",
        "    'Commercial': {\n",
        "        'IQR_Upper': commercial_upper,\n",
        "        '95th Percentile': commercial_percentiles[0.95],\n",
        "        '99th Percentile': commercial_percentiles[0.99]\n",
        "    },\n",
        "    'Special': {\n",
        "        'IQR_Upper': special_upper,\n",
        "        '95th Percentile': special_percentiles[0.95],\n",
        "        '99th Percentile': special_percentiles[0.99]\n",
        "    },\n",
        "    'Sports': {\n",
        "        'IQR_Upper': sports_upper,\n",
        "        '95th Percentile': sports_percentiles[0.95],\n",
        "        '99th Percentile': sports_percentiles[0.99]\n",
        "    }\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "thresholds_df = pd.DataFrame(thresholds).T\n",
        "\n",
        "thresholds_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGGr5UxPXtXo"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "Sports_mileage = cars[cars['vehicle_category'] == 'Sports']['mileage']\n",
        "\n",
        "def format_ticks(value, _):\n",
        "    return f'{int(value):,}'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(Sports_mileage.dropna(), bins=50, alpha=0.7, color='green', label='Sports')\n",
        "plt.title('Mileage Distribution for Sports Vehicles')\n",
        "plt.xlabel('Mileage')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.axvline(Sports_mileage.quantile(0.95), color='red', linestyle='--', label='95th Percentile')\n",
        "plt.axvline(Sports_mileage.quantile(0.99), color='green', linestyle='--', label='99th Percentile')\n",
        "\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y6dVdUWzg7WL"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "passenger_mileage = cars[cars['vehicle_category'] == 'Passenger']['mileage']\n",
        "\n",
        "def format_ticks(value, _):\n",
        "    return f'{int(value):,}'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(passenger_mileage.dropna(), bins=50, alpha=0.7, color='blue', label='Passenger')\n",
        "plt.title('Mileage Distribution for Passenger Vehicles')\n",
        "plt.xlabel('Mileage')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.axvline(passenger_mileage.quantile(0.95), color='red', linestyle='--', label='95th Percentile')\n",
        "plt.axvline(passenger_mileage.quantile(0.99), color='green', linestyle='--', label='99th Percentile')\n",
        "\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qY5row_wiPtN"
      },
      "outputs": [],
      "source": [
        "commercial_mileage = cars[cars['vehicle_category'] == 'Commercial']['mileage']\n",
        "\n",
        "def format_ticks(value, _):\n",
        "    return f'{int(value):,}'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(commercial_mileage.dropna(), bins=50, alpha=0.7, color='orange', label='Commercial')\n",
        "plt.title('Mileage Distribution for Commercial Vehicles')\n",
        "plt.xlabel('Mileage')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.axvline(commercial_mileage.quantile(0.95), color='red', linestyle='--', label='95th Percentile')\n",
        "plt.axvline(commercial_mileage.quantile(0.99), color='green', linestyle='--', label='99th Percentile')\n",
        "\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zOh0i6GjekOW"
      },
      "outputs": [],
      "source": [
        "special_mileage = cars[cars['vehicle_category'] == 'Special']['mileage']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(special_mileage.dropna(), bins=50, alpha=0.7, color='purple', label='Special')\n",
        "plt.title('Mileage Distribution for Special Vehicles')\n",
        "plt.xlabel('Mileage')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.axvline(special_mileage.quantile(0.95), color='red', linestyle='--', label='95th Percentile')\n",
        "plt.axvline(special_mileage.quantile(0.99), color='green', linestyle='--', label='99th Percentile')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZYP55nD8q0A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as mticker\n",
        "\n",
        "palette = sns.color_palette(\"Set2\")\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.boxplot(x='vehicle_category', y='mileage', data=cars, palette=palette, linewidth=2.5)\n",
        "\n",
        "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
        "\n",
        "plt.title('Mileage Distribution by Vehicle Category (Before Outlier Handling)', fontsize=16, fontweight='bold', color='darkblue')\n",
        "plt.ylabel('Mileage', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Vehicle Category', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.xticks(rotation=45, fontsize=12)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "for flier in ax.artists:\n",
        "    flier.set_alpha(0.7)\n",
        "\n",
        "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41proxtGXSR0"
      },
      "outputs": [],
      "source": [
        "base_upper_bounds = cars.groupby('vehicle_category')['mileage'].apply(\n",
        "    lambda x: x.quantile(0.99)\n",
        ")\n",
        "\n",
        "flexible_upper_bounds = base_upper_bounds * 1.2\n",
        "\n",
        "def replace_outliers_with_flexibility(row):\n",
        "    upper_bound = flexible_upper_bounds.get(row['vehicle_category'], None)\n",
        "    if upper_bound and row['mileage'] > upper_bound:\n",
        "        return upper_bound\n",
        "    return row['mileage']\n",
        "\n",
        "cars['mileage'] = cars.apply(replace_outliers_with_flexibility, axis=1)\n",
        "\n",
        "print(cars['mileage'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMYcxDhDUwF6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as mticker\n",
        "\n",
        "palette = sns.color_palette(\"Set2\")\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.boxplot(x='vehicle_category', y='mileage', data=cars, palette=palette, linewidth=2.5)\n",
        "\n",
        "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
        "\n",
        "plt.title('Mileage Distribution by Vehicle Category (After Outlier Handling)', fontsize=16, fontweight='bold', color='darkblue')\n",
        "plt.ylabel('Mileage', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Vehicle Category', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, fontsize=12)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "for flier in ax.artists:\n",
        "    flier.set_alpha(0.7)\n",
        "\n",
        "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q_c8GoHv0IU"
      },
      "outputs": [],
      "source": [
        "min_mileage = cars['mileage'].min()\n",
        "max_mileage = cars['mileage'].max()\n",
        "\n",
        "print(f\"Minimum Mileage: {min_mileage}\")\n",
        "print(f\"Maximum Mileage: {max_mileage}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EdOrbpI1YZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EC0NJJ0vCS0"
      },
      "outputs": [],
      "source": [
        "min_year = cars['year_of_registration'].min()\n",
        "max_year = cars['year_of_registration'].max()\n",
        "\n",
        "print(f\"Minimum year: {min_year}\")\n",
        "print(f\"Maximum year: {max_year}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuRPYasW2RQt"
      },
      "outputs": [],
      "source": [
        "missing_year_count = cars['year_of_registration'].isnull().sum()\n",
        "\n",
        "print(f\"Number of missing values in 'year_of_registration': {missing_year_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCrmQRNZRKP6"
      },
      "outputs": [],
      "source": [
        "correlation_price = cars['year_of_registration'].corr(cars['price'])\n",
        "correlation_mileage = cars['year_of_registration'].corr(cars['mileage'])\n",
        "\n",
        "print(f\"Correlation between year_of_registration and price: {correlation_price}\")\n",
        "print(f\"Correlation between year_of_registration and mileage: {correlation_mileage}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npot4KNBRVjb"
      },
      "outputs": [],
      "source": [
        "qualitative_columns = cars.select_dtypes(include=['object', 'bool']).columns\n",
        "\n",
        "relationship_strength = {}\n",
        "for column in qualitative_columns:\n",
        "    if cars[column].nunique() > 1:\n",
        "        category_means = cars.groupby(column)['year_of_registration'].mean()\n",
        "        overall_mean = cars['year_of_registration'].mean()\n",
        "        relationship_strength[column] = ((category_means - overall_mean) ** 2).sum()\n",
        "\n",
        "relationship_strength_sorted = sorted(relationship_strength.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for column, strength in relationship_strength_sorted:\n",
        "    print(f\"Column: {column}, Relationship Strength: {strength}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDZ1odFX3rVK"
      },
      "outputs": [],
      "source": [
        "grouped_years_model_make_mileage_condition = cars[cars['year_of_registration'].notnull()].groupby(\n",
        "    ['standard_model', 'standard_make', 'mileage_bins', 'vehicle_condition']\n",
        ")['year_of_registration'].median().round()\n",
        "\n",
        "def estimate_year_model_make_mileage_condition(row):\n",
        "    if pd.isnull(row['year_of_registration']):\n",
        "        return grouped_years_model_make_mileage_condition.get(\n",
        "            (row['standard_model'], row['standard_make'], row['mileage_bins'], row['vehicle_condition']),\n",
        "            None\n",
        "        )\n",
        "    return row['year_of_registration']\n",
        "\n",
        "cars['year_of_registration'] = cars.apply(estimate_year_model_make_mileage_condition, axis=1)\n",
        "\n",
        "remaining_missing_count = cars['year_of_registration'].isnull().sum()\n",
        "print(f\"Number of vehicles still missing year: {remaining_missing_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt6eNC1F3x6I"
      },
      "outputs": [],
      "source": [
        "grouped_years_model_mileage_condition = cars[cars['year_of_registration'].notnull()].groupby(\n",
        "    ['standard_model', 'mileage_bins', 'vehicle_condition']\n",
        ")['year_of_registration'].median().round()\n",
        "\n",
        "def estimate_year_model_mileage_condition(row):\n",
        "    if pd.isnull(row['year_of_registration']):\n",
        "        return grouped_years_model_mileage_condition.get(\n",
        "            (row['standard_model'], row['mileage_bins'], row['vehicle_condition']),\n",
        "            None\n",
        "        )\n",
        "    return row['year_of_registration']\n",
        "\n",
        "cars['year_of_registration'] = cars.apply(estimate_year_model_mileage_condition, axis=1)\n",
        "\n",
        "remaining_missing_count = cars['year_of_registration'].isnull().sum()\n",
        "print(f\"Number of vehicles still missing year: {remaining_missing_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x59P8ebW36j4"
      },
      "outputs": [],
      "source": [
        "grouped_years_make_mileage_condition = cars[cars['year_of_registration'].notnull()].groupby(\n",
        "    ['standard_make', 'mileage_bins', 'vehicle_condition']\n",
        ")['year_of_registration'].median().round()\n",
        "\n",
        "def estimate_year_make_mileage_condition(row):\n",
        "    if pd.isnull(row['year_of_registration']):\n",
        "        return grouped_years_make_mileage_condition.get(\n",
        "            (row['standard_make'], row['mileage_bins'], row['vehicle_condition']),\n",
        "            None\n",
        "        )\n",
        "    return row['year_of_registration']\n",
        "\n",
        "cars['year_of_registration'] = cars.apply(estimate_year_make_mileage_condition, axis=1)\n",
        "\n",
        "remaining_missing_count = cars['year_of_registration'].isnull().sum()\n",
        "print(f\"Number of vehicles still missing year: {remaining_missing_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80XnD_GJjiUe"
      },
      "outputs": [],
      "source": [
        "grouped_years_simple = cars[cars['year_of_registration'].notnull()].groupby(\n",
        "    ['standard_model', 'standard_make']\n",
        ")['year_of_registration'].median().round()\n",
        "\n",
        "def estimate_year_simple(row):\n",
        "    if pd.isnull(row['year_of_registration']):\n",
        "        return grouped_years_simple.get(\n",
        "            (row['standard_model'], row['standard_make']),\n",
        "            None\n",
        "        )\n",
        "    return row['year_of_registration']\n",
        "\n",
        "cars['year_of_registration'] = cars.apply(estimate_year_simple, axis=1)\n",
        "\n",
        "remaining_missing_count = cars['year_of_registration'].isnull().sum()\n",
        "print(f\"Number of vehicles still missing year: {remaining_missing_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsSAED5m99Zg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1D4T-YiGGhV"
      },
      "outputs": [],
      "source": [
        "cars_before_1960 = cars[cars['year_of_registration'] < 1960]\n",
        "\n",
        "before_1960_price_summary = cars_before_1960['price'].describe()\n",
        "\n",
        "full_price_summary = cars['price'].describe()\n",
        "\n",
        "print(\"Summary of prices for cars before 1960:\")\n",
        "print(before_1960_price_summary)\n",
        "print(\"\\nSummary of prices for the full dataset:\")\n",
        "print(full_price_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ang6ywktHB5r"
      },
      "outputs": [],
      "source": [
        "records_before_1960 = cars[cars['year_of_registration'] < 1960].shape[0]\n",
        "print(f\"Number of records before 1960: {records_before_1960}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZCnIg6kHQlp"
      },
      "outputs": [],
      "source": [
        "Q1 = cars['price'].quantile(0.25)\n",
        "Q3 = cars['price'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = cars[(cars['price'] < lower_bound) | (cars['price'] > upper_bound)]\n",
        "\n",
        "outliers_count = len(outliers)\n",
        "\n",
        "print(f\"Total Outliers in Price: {outliers_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuRuw-U1G97h"
      },
      "outputs": [],
      "source": [
        "outliers_by_make = outliers['standard_make'].value_counts().head(10)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "outliers_by_make.plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 10 Brands with Most Price Outliers')\n",
        "plt.xlabel('Brand (Standard Make)')\n",
        "plt.ylabel('Count of Outliers')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxP5bLbqHWxe"
      },
      "outputs": [],
      "source": [
        "Q1 = cars['price'].quantile(0.25)\n",
        "Q3 = cars['price'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = cars[(cars['price'] < lower_bound) | (cars['price'] > upper_bound)]\n",
        "\n",
        "outliers_count_by_model = outliers['standard_model'].value_counts()\n",
        "\n",
        "outliers_count_df = outliers_count_by_model.reset_index()\n",
        "outliers_count_df.columns = ['standard_model', 'outlier_count']\n",
        "\n",
        "outliers_count_df_sorted = outliers_count_df.sort_values(by='outlier_count', ascending=False)\n",
        "\n",
        "print(\"Outlier Counts by Model:\")\n",
        "print(outliers_count_df_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaH_UKiWoDS3"
      },
      "outputs": [],
      "source": [
        "Q1 = cars['price'].quantile(0.25)\n",
        "Q3 = cars['price'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = cars[(cars['price'] < lower_bound) | (cars['price'] > upper_bound)]\n",
        "\n",
        "outliers_count_by_model = outliers['standard_model'].value_counts()\n",
        "\n",
        "outliers_count_df = outliers_count_by_model.reset_index()\n",
        "outliers_count_df.columns = ['standard_model', 'outlier_count']\n",
        "\n",
        "outliers_count_df_sorted = outliers_count_df.sort_values(by='outlier_count', ascending=False)\n",
        "\n",
        "print(\"Price Outlier Counts by Model:\")\n",
        "print(outliers_count_df_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_4cxD73juFP"
      },
      "outputs": [],
      "source": [
        "luxury_threshold = cars['price'].quantile(0.99)\n",
        "print(luxury_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0KEfs57k_77"
      },
      "outputs": [],
      "source": [
        "luxury_threshold = cars['price'].quantile(0.99)\n",
        "print(luxury_threshold)\n",
        "\n",
        "cars['is_luxury'] = cars['price'].apply(lambda x: 1 if x > luxury_threshold else 0)\n",
        "\n",
        "luxury_count = cars['is_luxury'].sum()\n",
        "print(luxury_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qFgbb1deoAj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDjcolHLBD2L"
      },
      "outputs": [],
      "source": [
        "print(cars['fuel_type'].isnull().sum())\n",
        "null_percentage = (cars['fuel_type'].isnull().sum() / len(cars)) * 100\n",
        "print(f\"Percentage of missing fuel_type: {null_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHChrVzbAgcy"
      },
      "outputs": [],
      "source": [
        "print(cars['fuel_type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMtUop0Iqmzg"
      },
      "outputs": [],
      "source": [
        "numerical_columns = cars.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "relationship_strength_numeric_fuel = {}\n",
        "for column in numerical_columns:\n",
        "    if cars[column].notnull().sum() > 0:\n",
        "        category_means = cars.groupby('fuel_type')[column].mean()\n",
        "        overall_mean = cars[column].mean()\n",
        "        relationship_strength_numeric_fuel[column] = ((category_means - overall_mean) ** 2).sum()\n",
        "\n",
        "relationship_strength_numeric_fuel_sorted = sorted(relationship_strength_numeric_fuel.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Relationship strength with numerical columns:\")\n",
        "for column, strength in relationship_strength_numeric_fuel_sorted:\n",
        "    print(f\"{column}: {strength}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTyh6OfxqmqN"
      },
      "outputs": [],
      "source": [
        "qualitative_columns = cars.select_dtypes(include=['object', 'bool']).columns\n",
        "\n",
        "relationship_strength_fuel_qualitative = {}\n",
        "for column in qualitative_columns:\n",
        "    if cars[column].nunique() > 1:\n",
        "        category_modes = cars.groupby(column)['fuel_type'].apply(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "        overall_mode = cars['fuel_type'].mode()[0]\n",
        "        relationship_strength_fuel_qualitative[column] = ((category_modes != overall_mode).sum())\n",
        "\n",
        "relationship_strength_fuel_qualitative_sorted = sorted(relationship_strength_fuel_qualitative.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Relationship strength with qualitative columns:\")\n",
        "for column, strength in relationship_strength_fuel_qualitative_sorted:\n",
        "    print(f\"{column}: {strength}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-VfUHaQnHJd"
      },
      "outputs": [],
      "source": [
        "columns_to_group_by_fuel = ['standard_model', 'standard_make', 'year_of_registration']\n",
        "\n",
        "def fill_missing_fuel_model_make_year(group):\n",
        "    try:\n",
        "        return group.fillna(group.mode()[0]) if not group.mode().empty else group\n",
        "    except IndexError:\n",
        "        return group\n",
        "\n",
        "cars['fuel_type'] = cars.groupby(columns_to_group_by_fuel)['fuel_type'].transform(fill_missing_fuel_model_make_year)\n",
        "\n",
        "remaining_missing_fuel_type = cars['fuel_type'].isna().sum()\n",
        "print(f\"Remaining missing values in 'fuel_type' after grouping by model, make, and year: {remaining_missing_fuel_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVeT_X4frEMc"
      },
      "outputs": [],
      "source": [
        "columns_to_group_by_fuel = ['standard_model', 'year_of_registration']\n",
        "\n",
        "def fill_missing_fuel_model_year(group):\n",
        "    try:\n",
        "        return group.fillna(group.mode()[0]) if not group.mode().empty else group\n",
        "    except IndexError:\n",
        "        return group\n",
        "\n",
        "cars['fuel_type'] = cars.groupby(columns_to_group_by_fuel)['fuel_type'].transform(fill_missing_fuel_model_year)\n",
        "\n",
        "remaining_missing_fuel_type = cars['fuel_type'].isna().sum()\n",
        "print(f\"Remaining missing values in 'fuel_type' after grouping by model and year: {remaining_missing_fuel_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Kf3lGa2rSqq"
      },
      "outputs": [],
      "source": [
        "columns_to_group_by_fuel = ['standard_make', 'year_of_registration']\n",
        "\n",
        "def fill_missing_fuel_make_year(group):\n",
        "    try:\n",
        "        return group.fillna(group.mode()[0]) if not group.mode().empty else group\n",
        "    except IndexError:\n",
        "        return group\n",
        "\n",
        "cars['fuel_type'] = cars.groupby(columns_to_group_by_fuel)['fuel_type'].transform(fill_missing_fuel_make_year)\n",
        "\n",
        "remaining_missing_fuel_type = cars['fuel_type'].isna().sum()\n",
        "print(f\"Remaining missing values in 'fuel_type' after grouping by make and year: {remaining_missing_fuel_type}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQgCYkuSkbTH"
      },
      "outputs": [],
      "source": [
        "columns_to_group_by_fuel = ['year_of_registration', 'vehicle_category']\n",
        "\n",
        "def fill_missing_fuel_year_category(group):\n",
        "    try:\n",
        "        return group.fillna(group.mode()[0]) if not group.mode().empty else group\n",
        "    except IndexError:\n",
        "        return group\n",
        "\n",
        "cars['fuel_type'] = cars.groupby(columns_to_group_by_fuel)['fuel_type'].transform(fill_missing_fuel_year_category)\n",
        "\n",
        "remaining_missing_fuel_type = cars['fuel_type'].isna().sum()\n",
        "print(f\"Remaining missing fuel_type values after grouping by year and vehicle_category: {remaining_missing_fuel_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mygi0X7ogjxF"
      },
      "outputs": [],
      "source": [
        "columns_to_group_by_fuel = ['vehicle_category']\n",
        "\n",
        "def fill_missing_fuel_category(group):\n",
        "    try:\n",
        "        return group.fillna(group.mode()[0]) if not group.mode().empty else group\n",
        "    except IndexError:\n",
        "        return group\n",
        "\n",
        "cars['fuel_type'] = cars.groupby(columns_to_group_by_fuel)['fuel_type'].transform(fill_missing_fuel_category)\n",
        "\n",
        "remaining_missing_fuel_type = cars['fuel_type'].isna().sum()\n",
        "print(f\"Remaining missing fuel_type values after grouping by vehicle_category: {remaining_missing_fuel_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnJD43jE1S2g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_9X7qJrBAPM"
      },
      "outputs": [],
      "source": [
        "print(cars['standard_colour'].isnull().sum())\n",
        "null_percentage = (cars['standard_colour'].isnull().sum() / len(cars)) * 100\n",
        "print(f\"Percentage of missing standard_colour: {null_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFyABUxfBK9I"
      },
      "outputs": [],
      "source": [
        "print(cars['fuel_type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSgoLw6g1Ss7"
      },
      "outputs": [],
      "source": [
        "numerical_columns = cars.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "relationship_strength_numeric_colour = {}\n",
        "for column in numerical_columns:\n",
        "    if cars[column].notnull().sum() > 0:\n",
        "        category_means = cars.groupby('standard_colour')[column].mean()\n",
        "        overall_mean = cars[column].mean()\n",
        "        relationship_strength_numeric_colour[column] = ((category_means - overall_mean) ** 2).sum()\n",
        "\n",
        "relationship_strength_numeric_colour_sorted = sorted(relationship_strength_numeric_colour.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Relationship strength with numerical columns:\")\n",
        "for column, strength in relationship_strength_numeric_colour_sorted:\n",
        "    print(f\"{column}: {strength}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVQIPS1D1h46"
      },
      "outputs": [],
      "source": [
        "qualitative_columns = cars.select_dtypes(include=['object', 'bool']).columns\n",
        "\n",
        "relationship_strength_colour_qualitative = {}\n",
        "for column in qualitative_columns:\n",
        "    if cars[column].nunique() > 1:\n",
        "        category_modes = cars.groupby(column)['standard_colour'].apply(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
        "        overall_mode = cars['standard_colour'].mode()[0]\n",
        "        relationship_strength_colour_qualitative[column] = ((category_modes != overall_mode).sum())\n",
        "\n",
        "relationship_strength_colour_qualitative_sorted = sorted(relationship_strength_colour_qualitative.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Relationship strength with qualitative columns:\")\n",
        "for column, strength in relationship_strength_colour_qualitative_sorted:\n",
        "    print(f\"{column}: {strength}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY7KN-DiW7E6"
      },
      "outputs": [],
      "source": [
        "columns_to_group_by_colour = ['standard_model', 'standard_make', 'body_type']\n",
        "\n",
        "def fill_missing_colour(group):\n",
        "    try:\n",
        "        return group.fillna(group.mode()[0]) if not group.mode().empty else group\n",
        "    except IndexError:\n",
        "        return group\n",
        "\n",
        "cars['standard_colour'] = cars.groupby(columns_to_group_by_colour)['standard_colour'].transform(fill_missing_colour)\n",
        "\n",
        "remaining_missing_colour = cars['standard_colour'].isna().sum()\n",
        "print(f\"Remaining missing values in 'standard_colour' after grouping by model, make, and body type: {remaining_missing_colour}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spi-L_iqU02U"
      },
      "outputs": [],
      "source": [
        "columns_to_group_by_colour = ['standard_model', 'standard_make']\n",
        "\n",
        "def fill_missing_colour(group):\n",
        "    try:\n",
        "        return group.fillna(group.mode()[0]) if not group.mode().empty else group\n",
        "    except IndexError:\n",
        "        return group\n",
        "\n",
        "cars['standard_colour'] = cars.groupby(columns_to_group_by_colour)['standard_colour'].transform(fill_missing_colour)\n",
        "\n",
        "remaining_missing_colour = cars['standard_colour'].isna().sum()\n",
        "print(f\"Remaining missing values in 'standard_colour' after grouping by model and make: {remaining_missing_colour}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAEFRzAhXTiG"
      },
      "outputs": [],
      "source": [
        "columns_to_group_by_colour = ['body_type']\n",
        "\n",
        "def fill_missing_colour(group):\n",
        "    try:\n",
        "        return group.fillna(group.mode()[0]) if not group.mode().empty else group\n",
        "    except IndexError:\n",
        "        return group\n",
        "\n",
        "cars['standard_colour'] = cars.groupby(columns_to_group_by_colour)['standard_colour'].transform(fill_missing_colour)\n",
        "\n",
        "remaining_missing_colour = cars['standard_colour'].isna().sum()\n",
        "print(f\"Remaining missing values in 'standard_colour' after grouping by body type: {remaining_missing_colour}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn5bOqC1f2a4"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3JTrvUa1DQi"
      },
      "outputs": [],
      "source": [
        "cars.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0jXcMozW1CsC"
      },
      "outputs": [],
      "source": [
        "cars = cars.drop(columns=['public_reference', 'reg_code', 'mileage_bins', 'price_bins', 'vehicle_category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v6f7YMPB1cpt"
      },
      "outputs": [],
      "source": [
        "cars['body_type'] = cars.apply(\n",
        "    lambda row: 'Crossover' if row['crossover_car_and_van'] == True else row['body_type'], axis=1\n",
        "    )\n",
        "\n",
        "cars.drop(columns=['crossover_car_and_van'], inplace=True)\n",
        "\n",
        "print(cars['body_type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYe5ImrM1IIU"
      },
      "outputs": [],
      "source": [
        "records_before_2000 = cars[cars['year_of_registration'] < 2000].shape[0]\n",
        "\n",
        "print(f\"Number of records before 2000: {records_before_2000}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYh3qkrg1JEQ"
      },
      "outputs": [],
      "source": [
        "cars['is_old'] = cars['year_of_registration'].apply(lambda x: 1 if x < 2000 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TTDaLUw1W88"
      },
      "outputs": [],
      "source": [
        "old_cars_mean_price = cars[cars['is_old'] == 1]['price'].mean()\n",
        "new_cars_mean_price = cars[cars['is_old'] == 0]['price'].mean()\n",
        "print(f\"Average price for old cars: {old_cars_mean_price}\")\n",
        "print(f\"Average price for new cars: {new_cars_mean_price}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gifqA1EgEM8"
      },
      "outputs": [],
      "source": [
        "numeric_columns = cars.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "correlation_with_price = numeric_columns.corr()['price'].sort_values(ascending=False)\n",
        "\n",
        "print(correlation_with_price)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(numeric_columns.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQoLNM56z2J3"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "categorical_columns = ['vehicle_condition', 'standard_colour', 'standard_make', 'standard_model', 'fuel_type', 'body_type']\n",
        "\n",
        "for column in categorical_columns:\n",
        "    groups = [cars[cars[column] == category]['price'] for category in cars[column].unique()]\n",
        "    f_stat, p_value = f_oneway(*groups)\n",
        "    print(f\"Column: {column}, F-Statistic: {f_stat:.2f}, P-Value: {p_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMg-yz5pX-yC"
      },
      "outputs": [],
      "source": [
        "features_to_check = ['mileage', 'price']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "for i, feature in enumerate(features_to_check):\n",
        "    if feature == 'price':\n",
        "        data = cars[feature] / 1000\n",
        "        sns.kdeplot(data, shade=True, ax=axes[i], label='Price (in thousands)')\n",
        "    else:\n",
        "        sns.kdeplot(cars[feature], shade=True, ax=axes[i], label='Mileage')\n",
        "\n",
        "    axes[i].set_title(f\"Distribution of {feature}\")\n",
        "    axes[i].ticklabel_format(style='plain', axis='both')\n",
        "    axes[i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKbQ1s1Bh0fO"
      },
      "outputs": [],
      "source": [
        "cars['mileage_log'] = np.log1p(cars['mileage'])\n",
        "cars['price_log'] = np.log1p(cars['price'])\n",
        "\n",
        "cars.drop(columns=['mileage', 'price'], inplace=True)\n",
        "\n",
        "print(cars.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGYEVrvgYRmA"
      },
      "outputs": [],
      "source": [
        "features_to_check = ['mileage_log', 'price_log']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "for i, feature in enumerate(features_to_check):\n",
        "    if feature == 'price_log':\n",
        "        data = cars[feature] / 1000\n",
        "        sns.kdeplot(data, shade=True, ax=axes[i], label='Price (in thousands)')\n",
        "    else:\n",
        "        sns.kdeplot(cars[feature], shade=True, ax=axes[i], label='mileage_log')\n",
        "\n",
        "    axes[i].set_title(f\"Distribution of {feature}\")\n",
        "    axes[i].ticklabel_format(style='plain', axis='both')\n",
        "    axes[i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHrNY1PHbz2z"
      },
      "outputs": [],
      "source": [
        "cars['vehicle_condition'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX9IU7DujPu8"
      },
      "outputs": [],
      "source": [
        "cars['is_luxury'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6OQep29zZAG"
      },
      "source": [
        "# Encodings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLin7jIWJ5Q3"
      },
      "outputs": [],
      "source": [
        "cars.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1WAwPWC7GON"
      },
      "outputs": [],
      "source": [
        "# Apply One-Hot Encoding\n",
        "vehicle_condition_encoded = pd.get_dummies(cars['vehicle_condition'], prefix='vehicle_condition')\n",
        "\n",
        "is_luxury_encoded = pd.get_dummies(cars['is_luxury'], prefix='is_luxury')\n",
        "\n",
        "is_old_encoded = pd.get_dummies(cars['is_old'], prefix='is_old')\n",
        "\n",
        "cars = pd.concat([cars, vehicle_condition_encoded, is_luxury_encoded, is_old_encoded], axis=1)\n",
        "\n",
        "cars.drop(['vehicle_condition', 'is_luxury', 'is_old'], axis=1, inplace=True)\n",
        "\n",
        "cars.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wRLcSJx7HSy"
      },
      "outputs": [],
      "source": [
        "# Apply Target Encoding\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(cars, test_size=0.2, random_state=42)\n",
        "\n",
        "def target_encode(feature, target, train, full_data):\n",
        "    \"\"\"\n",
        "    Performs target encoding for a given feature and updates the full dataset.\n",
        "\n",
        "    Parameters:\n",
        "        feature: str, name of the column to encode\n",
        "        target: str, name of the target column\n",
        "        train: DataFrame, training set\n",
        "        full_data: DataFrame, full dataset to update\n",
        "\n",
        "    Returns:\n",
        "        Updates the full_data DataFrame with encoded values for the feature.\n",
        "    \"\"\"\n",
        "    category_means = train.groupby(feature)[target].mean()\n",
        "\n",
        "    full_data[f\"{feature}_encoded\"] = full_data[feature].map(category_means)\n",
        "\n",
        "    if full_data[f\"{feature}_encoded\"].isnull().any():\n",
        "        print(f\"Warning: Unknown categories in feature '{feature}' handled with global mean.\")\n",
        "    full_data[f\"{feature}_encoded\"].fillna(train[target].mean(), inplace=True)\n",
        "\n",
        "features_to_encode = ['standard_colour', 'fuel_type', 'standard_make', 'standard_model', 'body_type']\n",
        "\n",
        "for feature in features_to_encode:\n",
        "    target_encode(feature, 'price_log', train_data, train_data)\n",
        "    target_encode(feature, 'price_log', train_data, test_data)\n",
        "\n",
        "train_data.drop(columns=features_to_encode, inplace=True)\n",
        "test_data.drop(columns=features_to_encode, inplace=True)\n",
        "\n",
        "X_train = train_data.drop(columns=['price_log'])\n",
        "y_train = train_data['price_log']\n",
        "\n",
        "X_test = test_data.drop(columns=['price_log'])\n",
        "y_test = test_data['price_log']\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Test data shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfmhGD8vNd0j"
      },
      "outputs": [],
      "source": [
        "unknown_counts = {}\n",
        "for feature in ['standard_colour_encoded', 'fuel_type_encoded',\n",
        "                'standard_make_encoded', 'standard_model_encoded',\n",
        "                'body_type_encoded']:\n",
        "    if feature in test_data.columns:\n",
        "        unknown_categories = set(test_data[feature].unique()) - set(train_data[feature].unique())\n",
        "        unknown_counts[feature] = len(unknown_categories)\n",
        "\n",
        "print(\"Count of unknown categories in test data:\")\n",
        "print(unknown_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7XQIueQW-m"
      },
      "source": [
        "# **Part 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZu7ybtMwlfX"
      },
      "source": [
        "# Automated Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKlI8eAwNzjt"
      },
      "outputs": [],
      "source": [
        "X_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAjKxDRH--Le"
      },
      "outputs": [],
      "source": [
        "# RFECV (RandomForestRegressor)\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "rfecv = RFECV(estimator=lr_model, step=1, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "rfecv.fit(X_train, y_train)\n",
        "\n",
        "selected_features = X_train.columns[rfecv.support_]\n",
        "print(\"Selected features by RFECV:\", selected_features.tolist())\n",
        "print(\"Number of features selected:\", rfecv.n_features_)\n",
        "\n",
        "X_train_rfecv = rfecv.transform(X_train)\n",
        "X_test_rfecv = rfecv.transform(X_test)\n",
        "\n",
        "final_model = RandomForestRegressor(random_state=42)\n",
        "final_model.fit(X_train_rfecv, y_train)\n",
        "y_pred_rfecv = final_model.predict(X_test_rfecv)\n",
        "mae_rfecv = mean_absolute_error(y_test, y_pred_rfecv)\n",
        "\n",
        "error_factor = np.exp(mae_rfecv)\n",
        "relative_error_percent = (error_factor - 1) * 100\n",
        "\n",
        "print(f\"MAE with RFECV-selected features (log scale): {mae_rfecv:.2f}\")\n",
        "print(f\"Approx. real-world price error: {relative_error_percent:.1f}%\")\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(range(1, len(rfecv.cv_results_[\"mean_test_score\"]) + 1), -rfecv.cv_results_[\"mean_test_score\"], label='CV MAE')\n",
        "plt.axhline(y=mae_rfecv, color='red', linestyle='--', label='Test MAE = {:.2f}'.format(mae_rfecv))\n",
        "plt.xlabel(\"Number of features selected\")\n",
        "plt.ylabel(\"Mean Absolute Error\")\n",
        "plt.title(\"RFECV Cross-Validation vs Test Performance\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6obw1eYKAnw"
      },
      "outputs": [],
      "source": [
        "rf_full = RandomForestRegressor(random_state=42)\n",
        "rf_full.fit(X_train, y_train)\n",
        "y_pred_full = rf_full.predict(X_test)\n",
        "mae_full = mean_absolute_error(y_test, y_pred_full)\n",
        "percent_full = (np.exp(mae_full) - 1) * 100\n",
        "\n",
        "rf_rfecv = RandomForestRegressor(random_state=42)\n",
        "rf_rfecv.fit(X_train_rfecv, y_train)\n",
        "y_pred_rfecv = rf_rfecv.predict(X_test_rfecv)\n",
        "mae_rfecv = mean_absolute_error(y_test, y_pred_rfecv)\n",
        "percent_rfecv = (np.exp(mae_rfecv) - 1) * 100\n",
        "\n",
        "print(f\"MAE with all features (log scale): {mae_full:.3f} → approx. error: {percent_full:.1f}%\")\n",
        "print(f\"MAE with RFECV-selected features:    {mae_rfecv:.3f} → approx. error: {percent_rfecv:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_ranks = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Rank': rfecv.ranking_\n",
        "}).sort_values('Rank')\n",
        "\n",
        "\n",
        "print(feature_ranks)"
      ],
      "metadata": {
        "id": "145MrvsPZViJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Qj9aCVRBDH"
      },
      "source": [
        "# Tree Ensembles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsZ9xho8KXdg"
      },
      "outputs": [],
      "source": [
        "# Bagging (RandomForestRegressor)\n",
        "base_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 15],\n",
        "    'min_samples_split': [5, 10],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=base_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "y_train_pred = best_rf_model.predict(X_train)\n",
        "y_test_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "cv_r2_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5, scoring='r2')\n",
        "mean_cv_r2 = np.mean(cv_r2_scores)\n",
        "\n",
        "print(\"Results of Random Forest Regressor Model\")\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Mean CV R² Score: {mean_cv_r2:.3f}\")\n",
        "print(\"\\nTrain Metrics:\")\n",
        "print(f\"MAE: {mae_train:.3f} | MSE: {mse_train:.3f} | RMSE: {rmse_train:.3f} | R²: {r2_train:.3f}\")\n",
        "print(\"\\nTest Metrics:\")\n",
        "print(f\"MAE: {mae_test:.3f} | MSE: {mse_test:.3f} | RMSE: {rmse_test:.3f} | R²: {r2_test:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JU6FjktxOKv"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting Regressor\n",
        "base_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=base_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_gbr_model = grid_search.best_estimator_\n",
        "\n",
        "y_train_pred = best_gbr_model.predict(X_train)\n",
        "y_test_pred = best_gbr_model.predict(X_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "cv_r2_scores = cross_val_score(best_gbr_model, X_train, y_train, cv=5, scoring='r2')\n",
        "mean_cv_r2 = np.mean(cv_r2_scores)\n",
        "\n",
        "print(f\"Results of GradientBoostingRegressor Model\")\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Mean CV R² Score: {mean_cv_r2:.3f}\")\n",
        "\n",
        "print(\"\\nTrain Metrics:\")\n",
        "print(f\"MAE: {mae_train:.3f} | MSE: {mse_train:.3f} | RMSE: {rmse_train:.3f} | R²: {r2_train:.3f}\")\n",
        "\n",
        "print(\"\\nTest Metrics:\")\n",
        "print(f\"MAE: {mae_test:.3f} | MSE: {mse_test:.3f} | RMSE: {rmse_test:.3f} | R²: {r2_test:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5trThZT5S7Re"
      },
      "source": [
        "# Ensemble of Tree Ensembles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N7jTWdcxgBX"
      },
      "outputs": [],
      "source": [
        "# Voting (VotingRegressor)\n",
        "voting = VotingRegressor(estimators=[\n",
        "    ('rf', best_rf_model),\n",
        "    ('gb', best_gbr_model)\n",
        "])\n",
        "\n",
        "voting.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = voting.predict(X_train)\n",
        "y_test_pred = voting.predict(X_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Results of Voting Regressor\")\n",
        "print(\"\\nTrain Metrics:\")\n",
        "print(f\"MAE: {mae_train:.3f} | MSE: {mse_train:.3f} | RMSE: {rmse_train:.3f} | R²: {r2_train:.3f}\")\n",
        "\n",
        "print(\"\\nTest Metrics:\")\n",
        "print(f\"MAE: {mae_test:.3f} | MSE: {mse_test:.3f} | RMSE: {rmse_test:.3f} | R²: {r2_test:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p18Ex1kPxtaS"
      },
      "outputs": [],
      "source": [
        "# Stacking (StackingRegressor)\n",
        "estimators = [\n",
        "    ('rf', best_rf_model),\n",
        "    ('gb', best_gbr_model)\n",
        "]\n",
        "\n",
        "final_estimator = LinearRegression()\n",
        "\n",
        "stacking = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    passthrough=False,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stacking.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = stacking.predict(X_train)\n",
        "y_test_pred = stacking.predict(X_test)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Results of Stacking Regressor\")\n",
        "print(\"\\nTrain Metrics:\")\n",
        "print(f\"MAE: {mae_train:.3f} | MSE: {mse_train:.3f} | RMSE: {rmse_train:.3f} | R²: {r2_train:.3f}\")\n",
        "\n",
        "print(\"\\nTest Metrics:\")\n",
        "print(f\"MAE: {mae_test:.3f} | MSE: {mse_test:.3f} | RMSE: {rmse_test:.3f} | R²: {r2_test:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiMU_DJsBs6X"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "results = {}\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": best_rf_model,\n",
        "    \"Gradient Boosting\": best_gbr_model,\n",
        "    \"Voting Regressor\": voting,\n",
        "    \"Stacking Regressor\": stacking\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "    rmse_train = np.sqrt(mse_train)\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "    rmse_test = np.sqrt(mse_test)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        \"MAE Train\": round(mae_train, 3),\n",
        "        \"MAE Test\": round(mae_test, 3),\n",
        "        \"RMSE Train\": round(rmse_train, 3),\n",
        "        \"RMSE Test\": round(rmse_test, 3),\n",
        "        \"R² Train\": round(r2_train, 3),\n",
        "        \"R² Test\": round(r2_test, 3)\n",
        "    }\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = \"Model\"\n",
        "\n",
        "print(\"\\n Final Comparison Table (Train vs Test):\\n\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn4Qtlkfg6jk"
      },
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guhs_FHiEWgf"
      },
      "outputs": [],
      "source": [
        "rf_importances = pd.Series(best_rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
        "gb_importances = pd.Series(best_gbr_model.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
        "\n",
        "axes[0].barh(rf_importances.index, rf_importances.values, color='skyblue')\n",
        "axes[0].set_title(\"Random Forest Feature Importance\")\n",
        "axes[0].set_xlabel(\"Importance Score\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].barh(gb_importances.index, gb_importances.values, color='salmon')\n",
        "axes[1].set_title(\"Gradient Boosting Feature Importance\")\n",
        "axes[1].set_xlabel(\"Importance Score\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMkstOMVE0dL"
      },
      "outputs": [],
      "source": [
        "rf_perm = permutation_importance(best_rf_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "rf_sorted_idx = rf_perm.importances_mean.argsort()\n",
        "rf_labels = X_test.columns[rf_sorted_idx]\n",
        "rf_values = rf_perm.importances_mean[rf_sorted_idx]\n",
        "\n",
        "gb_perm = permutation_importance(best_gbr_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "gb_sorted_idx = gb_perm.importances_mean.argsort()\n",
        "gb_labels = X_test.columns[gb_sorted_idx]\n",
        "gb_values = gb_perm.importances_mean[gb_sorted_idx]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
        "\n",
        "axes[0].barh(rf_labels, rf_values, color='lightblue')\n",
        "axes[0].set_title(\"Permutation Importance – Random Forest\")\n",
        "axes[0].set_xlabel(\"Mean Importance\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].barh(gb_labels, gb_values, color='lightcoral')\n",
        "axes[1].set_title(\"Permutation Importance – Gradient Boosting\")\n",
        "axes[1].set_xlabel(\"Mean Importance\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnTDP3pJfzPx"
      },
      "source": [
        "# SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEoMfuV-DSgp"
      },
      "outputs": [],
      "source": [
        "X_test_sample = X_test.sample(n=1000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-UfMoU0FPj0"
      },
      "outputs": [],
      "source": [
        "for col in X_test_sample.select_dtypes(include='bool').columns:\n",
        "    X_test_sample[col] = X_test_sample[col].astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHLq1YX_NOPH"
      },
      "outputs": [],
      "source": [
        "explainer_rf = shap.TreeExplainer(best_rf_model)\n",
        "shap_values_rf = explainer_rf(X_test_sample)\n",
        "\n",
        "explainer_gb = shap.Explainer(best_gbr_model)\n",
        "shap_values_gb = explainer_gb(X_test_sample)\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_rf, X_test_sample, show=False)\n",
        "plt.title(\"Random Forest\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "buf1 = io.BytesIO()\n",
        "plt.savefig(buf1, format='png')\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_gb, X_test_sample, show=False)\n",
        "plt.title(\"Gradient Boosting\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "buf2 = io.BytesIO()\n",
        "plt.savefig(buf2, format='png')\n",
        "plt.close()\n",
        "\n",
        "img1 = Image.open(buf1)\n",
        "img2 = Image.open(buf2)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].imshow(img1)\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title(\"SHAP – Random Forest\")\n",
        "\n",
        "axes[1].imshow(img2)\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title(\"SHAP – Gradient Boosting\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcV0m8AgOsJJ"
      },
      "outputs": [],
      "source": [
        "mean_shap_rf = np.abs(shap_values_rf.values).mean(axis=0)\n",
        "shap_importance_rf = pd.Series(mean_shap_rf, index=X_test_sample.columns).sort_values()\n",
        "\n",
        "mean_shap_gb = np.abs(shap_values_gb.values).mean(axis=0)\n",
        "shap_importance_gb = pd.Series(mean_shap_gb, index=X_test_sample.columns).sort_values()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
        "\n",
        "axes[0].barh(shap_importance_rf.index, shap_importance_rf.values, color='skyblue')\n",
        "axes[0].set_title(\"Mean SHAP Value – Random Forest\")\n",
        "axes[0].set_xlabel(\"Average |SHAP value|\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].barh(shap_importance_gb.index, shap_importance_gb.values, color='salmon')\n",
        "axes[1].set_title(\"Mean SHAP Value – Gradient Boosting\")\n",
        "axes[1].set_xlabel(\"Average |SHAP value|\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsGV0hI7Xv7i"
      },
      "outputs": [],
      "source": [
        "instance = X_test_sample.iloc[0:1]\n",
        "\n",
        "# ========== RANDOM FOREST ==========\n",
        "explainer_rf = shap.TreeExplainer(best_rf_model)\n",
        "shap_values_rf = explainer_rf(instance)\n",
        "\n",
        "plt.figure(figsize=(12, 10))  # فضای کافی برای اسم‌ها\n",
        "shap.plots.waterfall(shap_values_rf[0], max_display=13, show=False)\n",
        "buf1 = io.BytesIO()\n",
        "plt.savefig(buf1, format='png', bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# ========== GRADIENT BOOSTING ==========\n",
        "explainer_gb = shap.TreeExplainer(best_gbr_model)\n",
        "shap_values_gb = explainer_gb(instance)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "shap.plots.waterfall(shap_values_gb[0], max_display=13, show=False)\n",
        "buf2 = io.BytesIO()\n",
        "plt.savefig(buf2, format='png', bbox_inches='tight')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe5gCkfxXwt_"
      },
      "outputs": [],
      "source": [
        "img1 = Image.open(buf1)\n",
        "img2 = Image.open(buf2)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "axes[0].imshow(img1)\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title(\"Waterfall – Random Forest\")\n",
        "\n",
        "axes[1].imshow(img2)\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title(\"Waterfall – Gradient Boosting\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSm7RtR7hVfX"
      },
      "source": [
        "# PDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYoWum1tZsxA"
      },
      "outputs": [],
      "source": [
        "features = ['year_of_registration', 'mileage_log']\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    best_gbr_model,\n",
        "    X_test,\n",
        "    features=features,\n",
        "    kind='average',\n",
        "    ax=ax,\n",
        "    grid_resolution=100\n",
        ")\n",
        "\n",
        "mileage_ax = ax[1]\n",
        "custom_mileage_vals = [1000, 5000, 10000, 20000, 50000, 100000, 200000]\n",
        "custom_ticks_log = np.log(custom_mileage_vals)\n",
        "custom_labels = ['1000', '5000', '10000', '20000', '50000', '100000', '200000']\n",
        "\n",
        "mileage_ax.set_xticks(custom_ticks_log)\n",
        "mileage_ax.set_xticklabels(custom_labels)\n",
        "mileage_ax.set_xlabel(\"Mileage (real km)\")\n",
        "\n",
        "fig.suptitle(\"Partial Dependence Plots – Gradient Boosting\", fontsize=14)\n",
        "for axis in ax:\n",
        "    axis.tick_params(axis='x', labelrotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcCHKLL2evY0"
      },
      "outputs": [],
      "source": [
        "features = ['year_of_registration', 'mileage_log']\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "display = PartialDependenceDisplay.from_estimator(\n",
        "    best_gbr_model,\n",
        "    X_test,\n",
        "    features=features,\n",
        "    kind='both',\n",
        "    ax=ax,\n",
        "    grid_resolution=100,\n",
        "    pd_line_kw = {'color': 'red'}\n",
        ")\n",
        "\n",
        "mileage_ax = ax[1]\n",
        "custom_mileage_vals = [1000, 5000, 10000, 20000, 50000, 100000, 200000]\n",
        "custom_ticks_log = np.log(custom_mileage_vals)\n",
        "custom_labels = ['1000', '5000', '10000', '20000', '50000', '100000', '200000']\n",
        "\n",
        "mileage_ax.set_xticks(custom_ticks_log)\n",
        "mileage_ax.set_xticklabels(custom_labels)\n",
        "mileage_ax.set_xlabel(\"Mileage\")\n",
        "\n",
        "fig.suptitle(\"Partial Dependence Plots with ICE – Gradient Boosting\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "\n",
        "for axis in ax:\n",
        "    axis.tick_params(axis='x', labelrotation=45)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClKD87qLZuBj"
      },
      "outputs": [],
      "source": [
        "features = ['year_of_registration', 'mileage_log']\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    best_rf_model,\n",
        "    X_test,\n",
        "    features=features,\n",
        "    kind='average',\n",
        "    ax=ax,\n",
        "    grid_resolution=100\n",
        ")\n",
        "\n",
        "mileage_ax = ax[1]\n",
        "custom_mileage_vals = [1000, 5000, 10000, 20000, 50000, 100000, 200000]\n",
        "custom_ticks_log = np.log(custom_mileage_vals)\n",
        "custom_labels = ['1000', '5000', '10000', '20000', '50000', '100000', '200000']\n",
        "\n",
        "mileage_ax.set_xticks(custom_ticks_log)\n",
        "mileage_ax.set_xticklabels(custom_labels)\n",
        "mileage_ax.set_xlabel(\"Mileage\")\n",
        "\n",
        "fig.suptitle(\"Partial Dependence Plots – Random Forest\", fontsize=14)\n",
        "for axis in ax:\n",
        "    axis.tick_params(axis='x', labelrotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9bs5SXveEGg"
      },
      "outputs": [],
      "source": [
        "features = ['year_of_registration', 'mileage_log']\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "display = PartialDependenceDisplay.from_estimator(\n",
        "    best_rf_model,\n",
        "    X_test,\n",
        "    features=features,\n",
        "    kind='both',\n",
        "    ax=ax,\n",
        "    grid_resolution=100,\n",
        "    pd_line_kw = {'color': 'red'}\n",
        ")\n",
        "\n",
        "mileage_ax = ax[1]\n",
        "custom_mileage_vals = [1000, 5000, 10000, 20000, 50000, 100000, 200000]\n",
        "custom_ticks_log = np.log(custom_mileage_vals)\n",
        "custom_labels = ['1000', '5000', '10000', '20000', '50000', '100000', '200000']\n",
        "\n",
        "mileage_ax.set_xticks(custom_ticks_log)\n",
        "mileage_ax.set_xticklabels(custom_labels)\n",
        "mileage_ax.set_xlabel(\"Mileage\")\n",
        "\n",
        "fig.suptitle(\"Partial Dependence Plots with ICE – Random Forest\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "\n",
        "for axis in ax:\n",
        "    axis.tick_params(axis='x', labelrotation=45)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6JMK5Rsvp-W"
      },
      "source": [
        "# **Part 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8LGlY3wv8uG"
      },
      "source": [
        "# Dimensionality Reduction (Linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwDvrxuawBqL"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({\n",
        "    'Column Name': X_train.columns,\n",
        "    'Data Type': [X_train[col].dtype for col in X_train.columns]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukdd2fslzFGx"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9v-hnHHzKnT"
      },
      "outputs": [],
      "source": [
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance by PCA Components')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMJ86jkf0AFu"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=8)\n",
        "X_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "print(\"Original shape:\", X_train_scaled.shape)\n",
        "print(\"Transformed shape:\", X_pca.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlrxKju20VIM"
      },
      "outputs": [],
      "source": [
        "loadings = pd.DataFrame(pca.components_.T,\n",
        "                        columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
        "                        index=X_train.columns)\n",
        "\n",
        "for i in range(pca.n_components_):\n",
        "    print(f\"\\n Top features for PC{i+1}:\")\n",
        "    print(loadings.iloc[:, i].abs().sort_values(ascending=False).head(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=8)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "results_pca = {}\n",
        "\n",
        "models_pca = {\n",
        "    \"Random Forest (PCA)\": RandomForestRegressor(random_state=42),\n",
        "    \"Gradient Boosting (PCA)\": GradientBoostingRegressor(random_state=42),\n",
        "    \"Voting Regressor (PCA)\": VotingRegressor(estimators=[\n",
        "        ('rf', RandomForestRegressor(random_state=42)),\n",
        "        ('gbr', GradientBoostingRegressor(random_state=42))\n",
        "    ]),\n",
        "    \"Stacking Regressor (PCA)\": StackingRegressor(estimators=[\n",
        "        ('rf', RandomForestRegressor(random_state=42)),\n",
        "        ('gbr', GradientBoostingRegressor(random_state=42))\n",
        "    ], final_estimator=RandomForestRegressor(random_state=42))\n",
        "}\n",
        "\n",
        "for name, model in models_pca.items():\n",
        "    model.fit(X_train_pca, y_train)\n",
        "    y_train_pred = model.predict(X_train_pca)\n",
        "    y_test_pred = model.predict(X_test_pca)\n",
        "\n",
        "    results_pca[name] = {\n",
        "        \"MAE Train\": round(mean_absolute_error(y_train, y_train_pred), 3),\n",
        "        \"MAE Test\": round(mean_absolute_error(y_test, y_test_pred), 3),\n",
        "        \"RMSE Train\": round(np.sqrt(mean_squared_error(y_train, y_train_pred)), 3),\n",
        "        \"RMSE Test\": round(np.sqrt(mean_squared_error(y_test, y_test_pred)), 3),\n",
        "        \"R² Train\": round(r2_score(y_train, y_train_pred), 3),\n",
        "        \"R² Test\": round(r2_score(y_test, y_test_pred), 3)\n",
        "    }\n",
        "\n",
        "results_pca_df = pd.DataFrame(results_pca).T\n",
        "results_pca_df.index.name = \"Model\"\n",
        "\n",
        "combined_results = pd.concat([results_df, results_pca_df])\n",
        "print(\"\\n📊 Final Comparison – Before and After PCA:\\n\")\n",
        "print(combined_results)"
      ],
      "metadata": {
        "id": "qCJ2q34TRt9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVqiHWj0wDDP"
      },
      "source": [
        "# Dimensionality Reduction (Non-Linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ0ilJKUBWqJ"
      },
      "source": [
        "# t_SNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 10000\n",
        "X_sample = X_train.sample(n=sample_size, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_sample_scaled = scaler.fit_transform(X_sample)"
      ],
      "metadata": {
        "id": "jgmahDVhjUDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8njmt181EC2"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X_sample_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=2)\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE Projection of Car Dataset')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z88_RS25v5P"
      },
      "outputs": [],
      "source": [
        "tsne_df = pd.DataFrame(X_tsne, columns=['t-SNE 1', 't-SNE 2'])\n",
        "tsne_df['year_of_registration'] = X_sample['year_of_registration'].values\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(\n",
        "    data=tsne_df,\n",
        "    x='t-SNE 1', y='t-SNE 2',\n",
        "    hue='year_of_registration',\n",
        "    palette='coolwarm',\n",
        "    s=60,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title('t-SNE Visualization Colored by Year of Registration')\n",
        "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbGGuGgni60-"
      },
      "outputs": [],
      "source": [
        "tsne_df = pd.DataFrame(X_tsne, columns=['t-SNE 1', 't-SNE 2'])\n",
        "\n",
        "mileage_real = np.exp(X_sample['mileage_log'].values)\n",
        "\n",
        "bins = [0, 5000, 20000, 50000, 100000, 200000, 500000]\n",
        "labels = ['<5000', '5000–20000', '20000–50000', '50000–100000', '100000–200000', '200000+']\n",
        "mileage_group = pd.cut(mileage_real, bins=bins, labels=labels)\n",
        "\n",
        "tsne_df['mileage_group'] = mileage_group\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(\n",
        "    data=tsne_df,\n",
        "    x='t-SNE 1', y='t-SNE 2',\n",
        "    hue='mileage_group',\n",
        "    palette='viridis',\n",
        "    s=60,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title('t-SNE Colored by Mileage Groups')\n",
        "plt.legend(title='Mileage Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ6QVkmhBKr4"
      },
      "source": [
        "# Isomap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwTdr6NXn_Hy"
      },
      "outputs": [],
      "source": [
        "sample_size = 10000\n",
        "X_sample = X_train.sample(n=sample_size, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_sample_scaled = scaler.fit_transform(X_sample)\n",
        "\n",
        "isomap = Isomap(n_neighbors=10, n_components=2)\n",
        "X_isomap = isomap.fit_transform(X_sample_scaled)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(X_isomap[:, 0], X_isomap[:, 1], s=2)\n",
        "plt.xlabel('Isomap Component 1')\n",
        "plt.ylabel('Isomap Component 2')\n",
        "plt.title('Isomap Projection of Car Dataset')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJYrFr5sqlHH"
      },
      "outputs": [],
      "source": [
        "isomap_df = pd.DataFrame(X_isomap, columns=['Isomap 1', 'Isomap 2'])\n",
        "isomap_df['year_of_registration'] = X_sample['year_of_registration'].values\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(\n",
        "    data=isomap_df,\n",
        "    x='Isomap 1',\n",
        "    y='Isomap 2',\n",
        "    hue='year_of_registration',\n",
        "    palette='coolwarm',\n",
        "    s=50,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title('Isomap Visualization Colored by Year of Registration')\n",
        "plt.xlabel('Isomap 1')\n",
        "plt.ylabel('Isomap 2')\n",
        "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz-td2IKBQIK"
      },
      "outputs": [],
      "source": [
        "n_components = 2\n",
        "\n",
        "isomap = Isomap(n_components=n_components)\n",
        "X_isomap = isomap.fit_transform(X_sample_scaled)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(\n",
        "    x=X_isomap[:, 0],\n",
        "    y=X_isomap[:, 1],\n",
        "    hue=X_sample['is_old_1'],\n",
        "    palette='plasma',\n",
        "    s=30\n",
        ")\n",
        "plt.title(\"Isomap Visualization Colored by is_old_1\")\n",
        "plt.xlabel(\"Isomap 1\")\n",
        "plt.ylabel(\"Isomap 2\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srialwMhowLU"
      },
      "outputs": [],
      "source": [
        "isomap_df = pd.DataFrame(X_isomap, columns=['Isomap 1', 'Isomap 2'])\n",
        "\n",
        "mileage_real = np.exp(X_sample['mileage_log'].values)\n",
        "\n",
        "bins = [0, 5000, 10000, 20000, 50000, 100000, 200000, np.inf]\n",
        "labels = ['<5000', '5000–10000', '10000–20000', '20000–50000', '50000–100000', '100000–200000', '200000+']\n",
        "isomap_df['Mileage Category'] = pd.cut(mileage_real, bins=bins, labels=labels)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(data=isomap_df, x='Isomap 1', y='Isomap 2', hue='Mileage Category', palette='viridis', s=50)\n",
        "plt.title('Isomap Visualization Colored by Mileage (Binned)')\n",
        "plt.xlabel('Isomap 1')\n",
        "plt.ylabel('Isomap 2')\n",
        "plt.legend(title='Mileage', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RusEhxRbwEKa"
      },
      "source": [
        "# Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NraaJPWP4V9H"
      },
      "outputs": [],
      "source": [
        "poly2 = PolynomialFeatures(degree=2)\n",
        "X_train_poly2 = poly2.fit_transform(X_train['mileage_log'].values.reshape(-1, 1))\n",
        "model2 = LinearRegression()\n",
        "model2.fit(X_train_poly2, y_train)\n",
        "\n",
        "poly4 = PolynomialFeatures(degree=4)\n",
        "X_train_poly4 = poly4.fit_transform(X_train['mileage_log'].values.reshape(-1, 1))\n",
        "model4 = LinearRegression()\n",
        "model4.fit(X_train_poly4, y_train)\n",
        "\n",
        "y_train_pred2 = model2.predict(X_train_poly2)\n",
        "y_train_pred4 = model4.predict(X_train_poly4)\n",
        "\n",
        "print(\"Polynomial Regression (Degree 2):\")\n",
        "print(f\"MAE: {mean_absolute_error(y_train, y_train_pred2):.4f}\")\n",
        "print(f\"MSE: {mean_squared_error(y_train, y_train_pred2):.4f}\")\n",
        "print(f\"R2 Score: {r2_score(y_train, y_train_pred2):.4f}\")\n",
        "\n",
        "print(\"\\n Polynomial Regression (Degree 4):\")\n",
        "print(f\"MAE: {mean_absolute_error(y_train, y_train_pred4):.4f}\")\n",
        "print(f\"MSE: {mean_squared_error(y_train, y_train_pred4):.4f}\")\n",
        "print(f\"R2 Score: {r2_score(y_train, y_train_pred4):.4f}\")\n",
        "\n",
        "mileage_range = np.linspace(X_train['mileage_log'].min(), X_train['mileage_log'].max(), 500).reshape(-1, 1)\n",
        "mileage_range_poly2 = poly2.transform(mileage_range)\n",
        "mileage_range_poly4 = poly4.transform(mileage_range)\n",
        "\n",
        "y_pred_poly2 = model2.predict(mileage_range_poly2)\n",
        "y_pred_poly4 = model4.predict(mileage_range_poly4)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X_train['mileage_log'], y_train, s=10, label=\"Data Points\")\n",
        "plt.plot(mileage_range, y_pred_poly2, color='red', label=\"Polynomial Regression (Degree 2)\")\n",
        "plt.plot(mileage_range, y_pred_poly4, color='green', label=\"Polynomial Regression (Degree 4)\")\n",
        "plt.xlabel('Mileage (log)')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Polynomial Regression Fit (Degree 2 vs Degree 4)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rSc3ThVSFl2"
      },
      "outputs": [],
      "source": [
        "selected_features = ['year_of_registration', 'standard_model_encoded', 'mileage_log']\n",
        "X_selected = X_train[selected_features]\n",
        "\n",
        "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly2 = poly2.fit_transform(X_selected)\n",
        "model2 = LinearRegression()\n",
        "model2.fit(X_train_poly2, y_train)\n",
        "\n",
        "poly4 = PolynomialFeatures(degree=4, include_bias=False)\n",
        "X_train_poly4 = poly4.fit_transform(X_selected)\n",
        "model4 = LinearRegression()\n",
        "model4.fit(X_train_poly4, y_train)\n",
        "\n",
        "y_train_pred2 = model2.predict(X_train_poly2)\n",
        "y_train_pred4 = model4.predict(X_train_poly4)\n",
        "\n",
        "print(\"Polynomial Regression with 3 Features (Degree 2):\")\n",
        "print(f\"MAE: {mean_absolute_error(y_train, y_train_pred2):.4f}\")\n",
        "print(f\"MSE: {mean_squared_error(y_train, y_train_pred2):.4f}\")\n",
        "print(f\"R2 Score: {r2_score(y_train, y_train_pred2):.4f}\")\n",
        "\n",
        "print(\"\\n Polynomial Regression with 3 Features (Degree 4):\")\n",
        "print(f\"MAE: {mean_absolute_error(y_train, y_train_pred4):.4f}\")\n",
        "print(f\"MSE: {mean_squared_error(y_train, y_train_pred4):.4f}\")\n",
        "print(f\"R2 Score: {r2_score(y_train, y_train_pred4):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97fCGKTteiXH"
      },
      "outputs": [],
      "source": [
        "selected_features = ['year_of_registration', 'standard_model_encoded', 'mileage_log',\n",
        "                     'standard_make_encoded', 'body_type_encoded']\n",
        "X_selected = X_train[selected_features]\n",
        "\n",
        "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly2 = poly2.fit_transform(X_selected)\n",
        "model2 = LinearRegression()\n",
        "model2.fit(X_train_poly2, y_train)\n",
        "\n",
        "poly4 = PolynomialFeatures(degree=4, include_bias=False)\n",
        "X_train_poly4 = poly4.fit_transform(X_selected)\n",
        "model4 = LinearRegression()\n",
        "model4.fit(X_train_poly4, y_train)\n",
        "\n",
        "y_train_pred2 = model2.predict(X_train_poly2)\n",
        "y_train_pred4 = model4.predict(X_train_poly4)\n",
        "\n",
        "print(\"Polynomial Regression with 5 Features (Degree 2):\")\n",
        "print(f\"MAE: {mean_absolute_error(y_train, y_train_pred2):.4f}\")\n",
        "print(f\"MSE: {mean_squared_error(y_train, y_train_pred2):.4f}\")\n",
        "print(f\"R2 Score: {r2_score(y_train, y_train_pred2):.4f}\")\n",
        "\n",
        "print(\"\\n Polynomial Regression with 5 Features (Degree 4):\")\n",
        "print(f\"MAE: {mean_absolute_error(y_train, y_train_pred4):.4f}\")\n",
        "print(f\"MSE: {mean_squared_error(y_train, y_train_pred4):.4f}\")\n",
        "print(f\"R2 Score: {r2_score(y_train, y_train_pred4):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0xdqjQjwIQW"
      },
      "source": [
        "# Clustering for Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_selected = X_train[['year_of_registration', 'mileage_log']].sample(n=10000, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "df_kmeans = X_selected.copy()\n",
        "df_kmeans['mileage'] = np.exp(df_kmeans['mileage_log'])\n",
        "df_kmeans['cluster_label'] = kmeans_labels\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(\n",
        "    data=df_kmeans,\n",
        "    x='mileage',\n",
        "    y='year_of_registration',\n",
        "    hue='cluster_label',\n",
        "    palette='tab10',\n",
        "    s=30\n",
        ")\n",
        "plt.title('KMeans Clusters Visualization (k=3)')\n",
        "plt.xlabel('Mileage')\n",
        "plt.ylabel('Year of Registration')\n",
        "plt.legend(title='Cluster')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YbdGJ1CjxC6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sample = X_train[['year_of_registration', 'mileage_log']].sample(n=10000, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_sample)\n",
        "\n",
        "k_range = range(2, 11)\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(k_range, inertias, marker='o', color='blue')\n",
        "axes[0].set_title('Elbow Method (Inertia)')\n",
        "axes[0].set_xlabel('Number of Clusters (k)')\n",
        "axes[0].set_ylabel('Inertia')\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(k_range, silhouette_scores, marker='o', color='red')\n",
        "axes[1].set_title('Silhouette Score')\n",
        "axes[1].set_xlabel('Number of Clusters (k)')\n",
        "axes[1].set_ylabel('Silhouette Score')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jKO-7Yt4-xGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 10000\n",
        "X_sample = X_train.sample(n=sample_size, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_sample_scaled = scaler.fit_transform(X_sample)\n",
        "\n",
        "linked = linkage(X_sample_scaled, method='ward')\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "dendrogram(\n",
        "    linked,\n",
        "    truncate_mode='lastp',\n",
        "    p=30,\n",
        "    leaf_rotation=90,\n",
        "    leaf_font_size=10,\n",
        "    show_contracted=True\n",
        ")\n",
        "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
        "plt.xlabel('Cluster Size')\n",
        "plt.ylabel('Distance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gx0TNNVsDzkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 3\n",
        "cluster_labels = fcluster(linked, n_clusters, criterion='maxclust')\n",
        "\n",
        "X_sample_with_clusters = X_sample.copy()\n",
        "X_sample_with_clusters['mileage'] = np.exp(X_sample_with_clusters['mileage_log'])  # تبدیل log به مقدار واقعی\n",
        "X_sample_with_clusters['cluster_label'] = cluster_labels\n",
        "\n",
        "print(X_sample_with_clusters['cluster_label'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(\n",
        "    X_sample_with_clusters['mileage'],\n",
        "    X_sample_with_clusters['year_of_registration'],\n",
        "    c=X_sample_with_clusters['cluster_label'],\n",
        "    cmap='viridis',\n",
        "    s=10\n",
        ")\n",
        "plt.xlabel('Mileage')\n",
        "plt.ylabel('Year of Registration')\n",
        "plt.title('Clusters Visualization')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "va5La79nEA0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOJLj9b_04bG"
      },
      "outputs": [],
      "source": [
        "X_selected = X_train[['year_of_registration', 'mileage_log']].sample(n=10000, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_selected_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.7, min_samples=10)\n",
        "dbscan_labels = dbscan.fit_predict(X_selected_scaled)\n",
        "\n",
        "df_dbscan = X_selected.copy()\n",
        "df_dbscan['mileage'] = np.exp(df_dbscan['mileage_log'])\n",
        "df_dbscan['cluster_label'] = dbscan_labels\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(\n",
        "    data=df_dbscan,\n",
        "    x='mileage',\n",
        "    y='year_of_registration',\n",
        "    hue='cluster_label',\n",
        "    palette='tab10',\n",
        "    s=30\n",
        ")\n",
        "plt.title('DBSCAN Clusters Visualization')\n",
        "plt.xlabel('Mileage')\n",
        "plt.ylabel('Year of Registration')\n",
        "plt.legend(title='Cluster')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bTEbwHeOYqks",
        "0Nzi5jWQnomk",
        "Hn5bOqC1f2a4",
        "U6OQep29zZAG"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}